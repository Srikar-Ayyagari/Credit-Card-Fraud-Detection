{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "cell_execution_strategy": "setup",
      "gpuType": "T4",
      "collapsed_sections": [
        "iifFiaYq9LrM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXtlZ7RnM5D_",
        "outputId": "6e33fda7-18a7-44dc-9dde-ffd9ef879ff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bayesian-optimization\n",
            "  Downloading bayesian_optimization-1.4.3-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from bayesian-optimization) (1.2.2)\n",
            "Collecting colorama>=0.4.6 (from bayesian-optimization)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18.0->bayesian-optimization) (3.2.0)\n",
            "Installing collected packages: colorama, bayesian-optimization\n",
            "Successfully installed bayesian-optimization-1.4.3 colorama-0.4.6\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2-cp310-cp310-manylinux2014_x86_64.whl (98.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.23.5)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.10/dist-packages (from catboost) (1.5.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from catboost) (1.10.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.10/dist-packages (from catboost) (5.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24->catboost) (2023.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (4.42.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->catboost) (3.1.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly->catboost) (8.2.3)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.2\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install bayesian-optimization\n",
        "!pip install catboost\n",
        "!pip install scikit-learn\n",
        "#!pip install autokeras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# basics\n",
        "from IPython import get_ipython\n",
        "get_ipython().magic('reset -sf')\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import scipy.stats as stats\n",
        "import seaborn as sns\n",
        "from glob import glob\n",
        "import os\n",
        "plt.style.use('ggplot')\n",
        "from bayes_opt import BayesianOptimization\n",
        "import tensorflow as tf\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "import matplotlib.patches as mpatches\n",
        "import time\n",
        "\n",
        "# preprocess\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "\n",
        "# Classifier Libraries\n",
        "import collections\n",
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "# preprocess\n",
        "from imblearn.pipeline import Pipeline, make_pipeline\n",
        "from imblearn import under_sampling\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import PowerTransformer\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from collections import Counter\n",
        "\n",
        "# Classifier Libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "import collections\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "from xgboost import XGBClassifier\n",
        "import xgboost as xgb\n",
        "import catboost as cgb\n",
        "from functools import partial\n",
        "\n",
        "#keras\n",
        "import tensorflow.keras.initializers\n",
        "import tensorflow.keras\n",
        "from tensorflow import keras\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "#from keras.wrappers.scikit_learn import KerasClassifier\n",
        "import statistics\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, InputLayer\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# resample\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn import under_sampling\n",
        "\n",
        "# imbalanced\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
        "from imblearn.metrics import classification_report_imbalanced\n",
        "\n",
        "# evalue\n",
        "from numpy import mean\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "from sklearn.metrics import average_precision_score\n",
        "from sklearn.metrics import log_loss\n",
        "#from sklearn.metrics import precision_recall_curve, plot_precision_recall_curve\n",
        "from sklearn.metrics import SCORERS\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "#from sklearn.metrics import roc_curve, plot_roc_curve\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# warning\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "4SeOcUVQNNPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#!ls 'drive/My Drive'"
      ],
      "metadata": {
        "id": "kZGL5njZNQcW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ad7623-5fc2-4494-fad4-f45f9dd577e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('/content/drive/MyDrive/Dissertation/creditcard.csv')\n",
        "print(\"Full dataset has\",data.shape[0], \"rows and\", data.shape[1], \"columns\")"
      ],
      "metadata": {
        "id": "sYaJniN1NTMF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56ce032-2332-4553-cb91-bed3cabb78b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full dataset has 284807 rows and 31 columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "uVfc8U4J9ZC_",
        "outputId": "3f67685d-1f0e-4915-ee97-eebfd3b74e04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
              "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
              "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
              "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
              "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
              "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
              "\n",
              "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
              "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
              "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
              "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
              "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
              "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
              "\n",
              "        V26       V27       V28  Amount  Class  \n",
              "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
              "1  0.125895 -0.008983  0.014724    2.69      0  \n",
              "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
              "3 -0.221929  0.062723  0.061458  123.50      0  \n",
              "4  0.502292  0.219422  0.215153   69.99      0  \n",
              "\n",
              "[5 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e6e36c22-9a4c-4a86-9f96-32d42e7d49cc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>V1</th>\n",
              "      <th>V2</th>\n",
              "      <th>V3</th>\n",
              "      <th>V4</th>\n",
              "      <th>V5</th>\n",
              "      <th>V6</th>\n",
              "      <th>V7</th>\n",
              "      <th>V8</th>\n",
              "      <th>V9</th>\n",
              "      <th>...</th>\n",
              "      <th>V21</th>\n",
              "      <th>V22</th>\n",
              "      <th>V23</th>\n",
              "      <th>V24</th>\n",
              "      <th>V25</th>\n",
              "      <th>V26</th>\n",
              "      <th>V27</th>\n",
              "      <th>V28</th>\n",
              "      <th>Amount</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>-1.359807</td>\n",
              "      <td>-0.072781</td>\n",
              "      <td>2.536347</td>\n",
              "      <td>1.378155</td>\n",
              "      <td>-0.338321</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.239599</td>\n",
              "      <td>0.098698</td>\n",
              "      <td>0.363787</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.018307</td>\n",
              "      <td>0.277838</td>\n",
              "      <td>-0.110474</td>\n",
              "      <td>0.066928</td>\n",
              "      <td>0.128539</td>\n",
              "      <td>-0.189115</td>\n",
              "      <td>0.133558</td>\n",
              "      <td>-0.021053</td>\n",
              "      <td>149.62</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.191857</td>\n",
              "      <td>0.266151</td>\n",
              "      <td>0.166480</td>\n",
              "      <td>0.448154</td>\n",
              "      <td>0.060018</td>\n",
              "      <td>-0.082361</td>\n",
              "      <td>-0.078803</td>\n",
              "      <td>0.085102</td>\n",
              "      <td>-0.255425</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.225775</td>\n",
              "      <td>-0.638672</td>\n",
              "      <td>0.101288</td>\n",
              "      <td>-0.339846</td>\n",
              "      <td>0.167170</td>\n",
              "      <td>0.125895</td>\n",
              "      <td>-0.008983</td>\n",
              "      <td>0.014724</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-1.358354</td>\n",
              "      <td>-1.340163</td>\n",
              "      <td>1.773209</td>\n",
              "      <td>0.379780</td>\n",
              "      <td>-0.503198</td>\n",
              "      <td>1.800499</td>\n",
              "      <td>0.791461</td>\n",
              "      <td>0.247676</td>\n",
              "      <td>-1.514654</td>\n",
              "      <td>...</td>\n",
              "      <td>0.247998</td>\n",
              "      <td>0.771679</td>\n",
              "      <td>0.909412</td>\n",
              "      <td>-0.689281</td>\n",
              "      <td>-0.327642</td>\n",
              "      <td>-0.139097</td>\n",
              "      <td>-0.055353</td>\n",
              "      <td>-0.059752</td>\n",
              "      <td>378.66</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>-0.966272</td>\n",
              "      <td>-0.185226</td>\n",
              "      <td>1.792993</td>\n",
              "      <td>-0.863291</td>\n",
              "      <td>-0.010309</td>\n",
              "      <td>1.247203</td>\n",
              "      <td>0.237609</td>\n",
              "      <td>0.377436</td>\n",
              "      <td>-1.387024</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.108300</td>\n",
              "      <td>0.005274</td>\n",
              "      <td>-0.190321</td>\n",
              "      <td>-1.175575</td>\n",
              "      <td>0.647376</td>\n",
              "      <td>-0.221929</td>\n",
              "      <td>0.062723</td>\n",
              "      <td>0.061458</td>\n",
              "      <td>123.50</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.0</td>\n",
              "      <td>-1.158233</td>\n",
              "      <td>0.877737</td>\n",
              "      <td>1.548718</td>\n",
              "      <td>0.403034</td>\n",
              "      <td>-0.407193</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.592941</td>\n",
              "      <td>-0.270533</td>\n",
              "      <td>0.817739</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009431</td>\n",
              "      <td>0.798278</td>\n",
              "      <td>-0.137458</td>\n",
              "      <td>0.141267</td>\n",
              "      <td>-0.206010</td>\n",
              "      <td>0.502292</td>\n",
              "      <td>0.219422</td>\n",
              "      <td>0.215153</td>\n",
              "      <td>69.99</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 31 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e6e36c22-9a4c-4a86-9f96-32d42e7d49cc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e6e36c22-9a4c-4a86-9f96-32d42e7d49cc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e6e36c22-9a4c-4a86-9f96-32d42e7d49cc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-613cb718-348b-4c9b-8204-5910f67f77ce\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-613cb718-348b-4c9b-8204-5910f67f77ce')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-613cb718-348b-4c9b-8204-5910f67f77ce button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load your dataset\n",
        "# data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Data Cleaning\n",
        "# Remove rows with missing values\n",
        "data_cleaned = data.dropna()\n",
        "\n",
        "# Remove duplicate rows\n",
        "data_cleaned = data_cleaned.drop_duplicates()\n",
        "\n",
        "# Prepare data for RFE\n",
        "X = data_cleaned.drop('Class', axis=1)\n",
        "y = data_cleaned['Class']\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize a model for RFE\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Initialize RFE\n",
        "rfe = RFE(estimator=model, n_features_to_select=10)  # Select the desired number of features\n",
        "\n",
        "# Fit RFE\n",
        "rfe.fit(X_train, y_train)\n",
        "\n",
        "# Get the selected features\n",
        "selected_feature_indices = rfe.support_\n",
        "selected_feature_names = X.columns[selected_feature_indices]\n",
        "\n",
        "print(\"Selected Features:\", selected_feature_names,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdxbKWPUEloC",
        "outputId": "12fbd282-4ac6-46d6-c00c-c0d834d248c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features: Index(['V4', 'V8', 'V10', 'V13', 'V14', 'V16', 'V21', 'V22', 'V26', 'V27'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(selected_feature_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xud9qi53FfmI",
        "outputId": "97f6a0f7-4545-4dea-ef1b-598fa0f2f897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load your dataset\n",
        "#data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Data Cleaning\n",
        "# Remove rows with missing values\n",
        "data_cleaned = data.dropna()\n",
        "\n",
        "# Remove duplicate rows\n",
        "data_cleaned = data_cleaned.drop_duplicates()\n",
        "\n",
        "# Feature Selection\n",
        "# Let's say you have a target column 'target_column'\n",
        "X = data_cleaned.drop('Class', axis=1)\n",
        "y = data_cleaned['Class']\n",
        "\n",
        "# You can use various techniques for feature selection, here's an example using correlation\n",
        "correlation_matrix = X.corr()\n",
        "#selected_features = correlation_matrix[correlation_matrix > 0.9]\n",
        "correlation_threshold = 0.2\n",
        "\n",
        "# Find pairs of features with high correlation\n",
        "highly_correlated_pairs = np.where(np.abs(correlation_matrix) > correlation_threshold)\n",
        "\n",
        "# Extract the unique feature indices from correlated pairs\n",
        "selected_feature_indices = set()\n",
        "for i, j in zip(*highly_correlated_pairs):\n",
        "    if i != j:\n",
        "        selected_feature_indices.add(i)\n",
        "        selected_feature_indices.add(j)\n",
        "\n",
        "# Convert the set of selected indices to a list\n",
        "selected_feature_indices = list(selected_feature_indices)\n",
        "\n",
        "# Get the selected feature names\n",
        "selected_feature_names = X.columns[selected_feature_indices]\n",
        "\n",
        "print(\"Selected Features:\", selected_feature_names)\n",
        "# Feature Engineering\n",
        "# Creating a new feature based on existing ones\n",
        "\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
      ],
      "metadata": {
        "id": "qAyF8hcx9LsA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c183bd59-c833-4a7e-80ec-353a20795ecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features: Index(['Time', 'V1', 'V2', 'V3', 'V5', 'V6', 'V7', 'V11', 'V20', 'V25',\n",
            "       'Amount'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(correlation_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUal-FJIrhhj",
        "outputId": "4dbab3ff-658f-4d40-ecf9-48511a1df313"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Time        V1        V2        V3        V4        V5        V6  \\\n",
            "Time    1.000000  0.117927 -0.010556 -0.422054 -0.105845  0.173223 -0.063279   \n",
            "V1      0.117927  1.000000  0.006875 -0.008112  0.002257 -0.007036  0.000413   \n",
            "V2     -0.010556  0.006875  1.000000  0.005278 -0.001495  0.005210 -0.000594   \n",
            "V3     -0.422054 -0.008112  0.005278  1.000000  0.002829 -0.006879 -0.001511   \n",
            "V4     -0.105845  0.002257 -0.001495  0.002829  1.000000  0.001744 -0.000880   \n",
            "V5      0.173223 -0.007036  0.005210 -0.006879  0.001744  1.000000 -0.000938   \n",
            "V6     -0.063279  0.000413 -0.000594 -0.001511 -0.000880 -0.000938  1.000000   \n",
            "V7      0.085335 -0.009173  0.007425 -0.011721  0.004657 -0.008709  0.000436   \n",
            "V8     -0.038203 -0.001168  0.002899 -0.001815  0.000890  0.001430  0.003036   \n",
            "V9     -0.007861  0.001828 -0.000274 -0.003579  0.002154 -0.001213 -0.000734   \n",
            "V10     0.031068  0.000815  0.000620 -0.009632  0.002753 -0.006050 -0.002180   \n",
            "V11    -0.248536  0.001028 -0.000633  0.002339 -0.001223  0.000411 -0.000211   \n",
            "V12     0.125500 -0.001524  0.002266 -0.005900  0.003366 -0.002342 -0.001185   \n",
            "V13    -0.065958 -0.000568  0.000680  0.000113  0.000177  0.000019  0.000397   \n",
            "V14    -0.100316 -0.002663  0.002711 -0.003027  0.002801 -0.001000  0.000184   \n",
            "V15    -0.184392 -0.000602  0.001538 -0.001230  0.000572 -0.001171 -0.000470   \n",
            "V16     0.011286 -0.003345  0.004013 -0.004430  0.003346 -0.002373  0.000122   \n",
            "V17    -0.073819 -0.003491  0.003244 -0.008159  0.003655 -0.004466 -0.001716   \n",
            "V18     0.090305 -0.003535  0.002477 -0.003495  0.002325 -0.002685  0.000541   \n",
            "V19     0.029537  0.000919 -0.000358 -0.000016 -0.000560  0.000436  0.000106   \n",
            "V20    -0.051022 -0.001393 -0.001287 -0.002269  0.000318 -0.001185 -0.000181   \n",
            "V21     0.045913  0.002818 -0.004897  0.003500 -0.001034  0.001622 -0.002134   \n",
            "V22     0.143727 -0.001436  0.001237 -0.000275  0.000115 -0.000559  0.001104   \n",
            "V23     0.051474 -0.001330 -0.003855  0.000449  0.000732  0.001183 -0.000755   \n",
            "V24    -0.015954 -0.000723  0.000701 -0.000072 -0.000120  0.000198  0.001202   \n",
            "V25    -0.233262 -0.000222 -0.001569  0.000425  0.000162  0.000069  0.000697   \n",
            "V26    -0.041818 -0.000684  0.000253 -0.000094  0.000777  0.000390 -0.000028   \n",
            "V27    -0.005171 -0.015706  0.007555 -0.007051  0.001322 -0.005798  0.000289   \n",
            "V28    -0.009305 -0.004861  0.001611 -0.000134  0.000231 -0.000820  0.000925   \n",
            "Amount -0.010559 -0.230105 -0.533428 -0.212410  0.099514 -0.387685  0.216389   \n",
            "Class  -0.012359 -0.094486  0.084624 -0.182322  0.129326 -0.087812 -0.043915   \n",
            "\n",
            "              V7        V8        V9  ...       V21       V22       V23  \\\n",
            "Time    0.085335 -0.038203 -0.007861  ...  0.045913  0.143727  0.051474   \n",
            "V1     -0.009173 -0.001168  0.001828  ...  0.002818 -0.001436 -0.001330   \n",
            "V2      0.007425  0.002899 -0.000274  ... -0.004897  0.001237 -0.003855   \n",
            "V3     -0.011721 -0.001815 -0.003579  ...  0.003500 -0.000275  0.000449   \n",
            "V4      0.004657  0.000890  0.002154  ... -0.001034  0.000115  0.000732   \n",
            "V5     -0.008709  0.001430 -0.001213  ...  0.001622 -0.000559  0.001183   \n",
            "V6      0.000436  0.003036 -0.000734  ... -0.002134  0.001104 -0.000755   \n",
            "V7      1.000000 -0.006419 -0.004921  ...  0.009010 -0.002280  0.003303   \n",
            "V8     -0.006419  1.000000  0.001038  ...  0.018892 -0.006156  0.004994   \n",
            "V9     -0.004921  0.001038  1.000000  ...  0.000679  0.000785  0.000677   \n",
            "V10    -0.013617  0.000481 -0.012613  ...  0.003777 -0.000481  0.001917   \n",
            "V11     0.002454  0.004688 -0.000217  ... -0.002760 -0.000150 -0.000037   \n",
            "V12    -0.006153 -0.004414 -0.002385  ...  0.003285  0.000151  0.000486   \n",
            "V13    -0.000170 -0.001381  0.000745  ...  0.000522  0.000016  0.000252   \n",
            "V14    -0.003816 -0.008387  0.001981  ...  0.005633 -0.001906  0.000666   \n",
            "V15    -0.001394  0.001044 -0.000283  ... -0.000271 -0.001197  0.000969   \n",
            "V16    -0.005944 -0.004376 -0.000086  ...  0.004326 -0.000820  0.001209   \n",
            "V17    -0.008794 -0.005576 -0.002318  ...  0.003560 -0.000162  0.000667   \n",
            "V18    -0.004279 -0.001323 -0.000373  ...  0.001629 -0.000533  0.000240   \n",
            "V19     0.000846 -0.000626  0.000247  ...  0.000244  0.001342  0.000381   \n",
            "V20    -0.001192  0.000271 -0.001838  ...  0.005372 -0.001617 -0.001094   \n",
            "V21     0.009010  0.018892  0.000679  ...  1.000000  0.009645 -0.006391   \n",
            "V22    -0.002280 -0.006156  0.000785  ...  0.009645  1.000000  0.001929   \n",
            "V23     0.003303  0.004994  0.000677  ... -0.006391  0.001929  1.000000   \n",
            "V24    -0.000384  0.000113 -0.000103  ...  0.001210 -0.000031  0.000273   \n",
            "V25    -0.000072  0.000011 -0.000275  ... -0.000872  0.000197 -0.000532   \n",
            "V26     0.000624 -0.001407  0.001253  ... -0.000874 -0.001495 -0.000185   \n",
            "V27    -0.004537  0.000613  0.008221  ... -0.005216  0.003037 -0.002028   \n",
            "V28     0.001657 -0.000099  0.005591  ... -0.004436  0.001392 -0.003224   \n",
            "Amount  0.400408 -0.104662 -0.044123  ...  0.108058 -0.064965 -0.112833   \n",
            "Class  -0.172347  0.033068 -0.094021  ...  0.026357  0.004887 -0.006333   \n",
            "\n",
            "             V24       V25       V26       V27       V28    Amount     Class  \n",
            "Time   -0.015954 -0.233262 -0.041818 -0.005171 -0.009305 -0.010559 -0.012359  \n",
            "V1     -0.000723 -0.000222 -0.000684 -0.015706 -0.004861 -0.230105 -0.094486  \n",
            "V2      0.000701 -0.001569  0.000253  0.007555  0.001611 -0.533428  0.084624  \n",
            "V3     -0.000072  0.000425 -0.000094 -0.007051 -0.000134 -0.212410 -0.182322  \n",
            "V4     -0.000120  0.000162  0.000777  0.001322  0.000231  0.099514  0.129326  \n",
            "V5      0.000198  0.000069  0.000390 -0.005798 -0.000820 -0.387685 -0.087812  \n",
            "V6      0.001202  0.000697 -0.000028  0.000289  0.000925  0.216389 -0.043915  \n",
            "V7     -0.000384 -0.000072  0.000624 -0.004537  0.001657  0.400408 -0.172347  \n",
            "V8      0.000113  0.000011 -0.001407  0.000613 -0.000099 -0.104662  0.033068  \n",
            "V9     -0.000103 -0.000275  0.001253  0.008221  0.005591 -0.044123 -0.094021  \n",
            "V10     0.000154 -0.000565  0.001089  0.010769  0.009159 -0.102255 -0.206971  \n",
            "V11     0.000080  0.000047 -0.000204  0.001987  0.002562 -0.000015  0.149067  \n",
            "V12     0.000588 -0.000181 -0.000138 -0.000929 -0.000613 -0.009254 -0.250711  \n",
            "V13    -0.000049  0.000248 -0.000101 -0.001577 -0.000604  0.005209 -0.003897  \n",
            "V14    -0.000026  0.000155 -0.000702 -0.004556 -0.004664  0.034122 -0.293375  \n",
            "V15     0.000113  0.000445 -0.002034 -0.000641  0.000858 -0.003265 -0.003300  \n",
            "V16    -0.000482  0.000215 -0.001245 -0.003974 -0.001629 -0.004488 -0.187186  \n",
            "V17     0.001006 -0.000685  0.000157 -0.003421 -0.002703  0.007730 -0.313498  \n",
            "V18    -0.000710 -0.000559 -0.000596 -0.004231 -0.001256  0.035775 -0.105340  \n",
            "V19    -0.000112 -0.000084  0.000856 -0.000544  0.000353 -0.055994  0.033631  \n",
            "V20    -0.000303 -0.000643 -0.000310 -0.000049  0.002671  0.340729  0.021486  \n",
            "V21     0.001210 -0.000872 -0.000874 -0.005216 -0.004436  0.108058  0.026357  \n",
            "V22    -0.000031  0.000197 -0.001495  0.003037  0.001392 -0.064965  0.004887  \n",
            "V23     0.000273 -0.000532 -0.000185 -0.002028 -0.003224 -0.112833 -0.006333  \n",
            "V24     1.000000 -0.000188  0.000568 -0.000885  0.000322  0.005055 -0.007210  \n",
            "V25    -0.000188  1.000000  0.000048 -0.001339 -0.000565 -0.047596  0.003202  \n",
            "V26     0.000568  0.000048  1.000000 -0.003294 -0.000999 -0.003425  0.004265  \n",
            "V27    -0.000885 -0.001339 -0.003294  1.000000 -0.013950  0.027922  0.021892  \n",
            "V28     0.000322 -0.000565 -0.000999 -0.013950  1.000000  0.010143  0.009682  \n",
            "Amount  0.005055 -0.047596 -0.003425  0.027922  0.010143  1.000000  0.005777  \n",
            "Class  -0.007210  0.003202  0.004265  0.021892  0.009682  0.005777  1.000000  \n",
            "\n",
            "[31 rows x 31 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_feature_columns = selected_features.columns.tolist()\n",
        "print(selected_feature_columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "GElW-DH798EV",
        "outputId": "eff7d994-a6eb-4916-fa63-033537eed9a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e03608fee903>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mselected_feature_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_feature_columns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'selected_features' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LassoCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load your dataset\n",
        "# data = pd.read_csv('your_dataset.csv')\n",
        "\n",
        "# Data Cleaning\n",
        "# Remove rows with missing values\n",
        "data_cleaned = data.dropna()\n",
        "\n",
        "# Remove duplicate rows\n",
        "data_cleaned = data_cleaned.drop_duplicates()\n",
        "\n",
        "# Feature Selection\n",
        "X = data_cleaned.drop('Class', axis=1)\n",
        "y = data_cleaned['Class']\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Perform Lasso regression for feature selection\n",
        "lasso = LassoCV(cv=5)  # You can specify cross-validation folds\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "# Get the selected features\n",
        "selected_feature_indices = np.where(lasso.coef_ != 0)[0]\n",
        "selected_feature_names = X.columns[selected_feature_indices]\n",
        "\n",
        "print(\"Selected Features:\", selected_feature_names)\n",
        "\n",
        "# Evaluate the performance of the selected features on the validation set\n",
        "X_val_selected = X_val.iloc[:, selected_feature_indices]\n",
        "# Train and evaluate your model using the selected features\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VfjSIORor56j",
        "outputId": "d8b98036-3c73-451d-ddb2-fb3e593541f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected Features: Index(['Time'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Assuming X_train, y_train are your training data\n",
        "k = 10  # Number of top features to select\n",
        "\n",
        "# Initialize SelectKBest with ANOVA F-value as score function\n",
        "selector = SelectKBest(score_func=f_classif, k=k)\n",
        "\n",
        "# Fit selector on training data and transform the data\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "\n",
        "# Get the indices of selected features\n",
        "selected_feature_indices = selector.get_support(indices=True)\n",
        "\n",
        "# Subset the original feature names based on selected indices\n",
        "selected_feature_names = X_train.columns[selected_feature_indices]\n",
        "\n",
        "# Use the selected features in your further analysis\n"
      ],
      "metadata": {
        "id": "TR9lr4h7-wis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(selected_feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5lHc-h2-zj1",
        "outputId": "60b6ad8d-c87d-4f63-f26e-ae80f9fdc745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['V3', 'V4', 'V7', 'V10', 'V11', 'V12', 'V14', 'V16', 'V17', 'V18'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_n = X_train[selected_feature_names]\n",
        "X_test_n = X_test[selected_feature_names]"
      ],
      "metadata": {
        "id": "eP-lJmTV-5Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Sigmoid function\n",
        "def sigmoid(x):\n",
        "    return (1/(1+np.exp(-x)))\n",
        "#Rectified Linear unit\n",
        "def relu(x):\n",
        "    return np.maximum(x, 0., None)"
      ],
      "metadata": {
        "id": "1ljnWH-7Np_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the Duplicate Values\n",
        "data.drop_duplicates(inplace = True)"
      ],
      "metadata": {
        "id": "4JuN20QUNwno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for duplicate data if they exist\n",
        "print(data.duplicated().value_counts())\n",
        "\n",
        "# Reset the index\n",
        "data.reset_index(drop = True , inplace = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APmbeMhDN1Xk",
        "outputId": "418c5fa3-d68c-45f1-c97a-e770c60aa294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False    283726\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data)#, columns=['V10' ,'V11' ,'V12' ,'V14' ,'V16' ,'V17', 'Class'])"
      ],
      "metadata": {
        "id": "9NM1oKAGN3LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "print(X.shape,y.shape)"
      ],
      "metadata": {
        "id": "BG5N7oCEN9sE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a437f013-a8fa-41c4-9d21-7f009edf61fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(283726, 30) (283726,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C3BwCFSrARV",
        "outputId": "f8d19927-8614-43b4-8e5f-e790d9cb2ca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Features : ['V10' 'V11' 'V12' 'V14' 'V16' 'V17']\n",
            "Bad Features : ['Time' 'V1' 'V2' 'V3' 'V4' 'V5' 'V6' 'V7' 'V8' 'V9' 'V13' 'V15' 'V18'\n",
            " 'V19' 'V20' 'V21' 'V22' 'V23' 'V24' 'V25' 'V26' 'V27' 'V28' 'Amount']\n"
          ]
        }
      ],
      "source": [
        "#set of best features set\n",
        "from sklearn.datasets import make_classification\n",
        "from collections import Counter\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=.20)\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest,f_classif,mutual_info_classif\n",
        "infogain_classif = SelectKBest(score_func=mutual_info_classif, k=6)\n",
        "infogain_classif.fit(X_train, y_train)\n",
        "mask=infogain_classif.get_support()\n",
        "not_mask=np.logical_not(mask)\n",
        "\n",
        "all_features=np.array(list(X))\n",
        "\n",
        "best_features=all_features[mask]\n",
        "bad_features=all_features[~mask]#not_mask\n",
        "print('Best Features :',best_features)\n",
        "print('Bad Features :',bad_features)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(data, columns=['V10', 'V11', 'V12' ,'V14' ,'V16' ,'V17', 'Class']) #Replace the columns with the best features from prev block\n",
        "print(df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jP-Kt0eQwtWz",
        "outputId": "95bd2d1d-fa28-4703-c1a7-0a06140beb29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(283726, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop('Class', axis=1)\n",
        "y = df['Class']\n",
        "print(X.shape,y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6zw7379wzPi",
        "outputId": "4fe600c4-3d79-475c-94cc-9aa930e6d5a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(283726, 6) (283726,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = train_test_split(df, test_size = 0.2, random_state=42)\n",
        "#train, test = train_test_split(df, test_size = 0.2, shuffle=True, stratify=df['Class'])\n",
        "print(\"Train Shape: {} \\nTest Shape: {}\".format(train.shape, test.shape))\n",
        "total = train.shape[0]\n",
        "print(total)"
      ],
      "metadata": {
        "id": "RnwnC8r0ODvm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75e7ec3d-b7c8-4695-f509-414c90b9ca9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Shape: (226980, 7) \n",
            "Test Shape: (56746, 7)\n",
            "226980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train  = train.drop('Class', axis=1).values\n",
        "y_train  = train['Class'].values\n",
        "print('before sampling: \\nX_train  shape: {} \\ny_train shape: {}'.format(X_train.shape, y_train.shape))\n",
        "\n",
        "X_test  = test.drop('Class', axis=1).values\n",
        "y_test  = test['Class'].values\n",
        "print('\\nX_test  shape: {} \\ny_test shape: {}'.format(X_test.shape, y_test.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "143mhCJxw7Bq",
        "outputId": "761b119d-2d0a-4b6e-af00-3016dede69ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before sampling: \n",
            "X_train  shape: (226980, 6) \n",
            "y_train shape: (226980,)\n",
            "\n",
            "X_test  shape: (56746, 6) \n",
            "y_test shape: (56746,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_n = X_train_n.values\n",
        "y_train = y_train.values\n",
        "X_test_n = X_test_n.values\n",
        "y_test = y_test.values"
      ],
      "metadata": {
        "id": "SmIJtEG0_XQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "original_sequence_length = 6\n",
        "target_sequence_length = 256\n",
        "\n",
        "# Calculate the maximum number of sequences that can be created without exceeding the original dataset size\n",
        "max_sequences = len(X_train) * original_sequence_length // target_sequence_length\n",
        "\n",
        "print(\"Maximum sequences that can be created:\", max_sequences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSD5qun60cqS",
        "outputId": "57f22d51-dd78-4d98-c555-a03512d7a56f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum sequences that can be created: 5319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_reshaped = X_train.reshape(-1, 6, 1)\n",
        "X_test_reshaped = X_test.reshape(-1, 6, 1)\n"
      ],
      "metadata": {
        "id": "fo4K70iizykL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_timesteps = 1\n",
        "n_features = 10\n",
        "X_train_new = X_train_n.reshape(-1,1, n_features)\n",
        "#X_train_new = X_train_resampled_loaded.reshape(-1,1, n_features)\n",
        "#n_samples = y_train_resampled_loaded.shape[0]\n",
        "y_train_new = y_train.reshape(-1, 1, 1)\n",
        "#y_train_new = y_train_resampled_loaded.reshape(n_samples, 1, 1)\n",
        "print(X_train_new.shape , y_train_new.shape)\n",
        "X_test_new = X_test_n.reshape(-1,1,n_features)\n",
        "y_test_new = y_test.reshape(-1,1,1)\n",
        "#X_test_new = X_test_resampled.reshape(-1,1,n_features)\n",
        "#y_test_new = y_test_resampled.reshape(-1,1,1)\n",
        "print(X_test_new.shape , y_test_new.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEClC0D5O6CZ",
        "outputId": "5103c090-7fd5-4c51-b2a6-55c1072d7d6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(198608, 1, 10) (198608, 1, 1)\n",
            "(85118, 1, 10) (85118, 1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#######\n",
        "#######\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Reshape and normalize X_train\n",
        "X_train_standardized = X_train.reshape(-1, n_timesteps * n_features)\n",
        "X_train_standardized = scaler.fit_transform(X_train_standardized)\n",
        "X_train_standardized = X_train_standardized.reshape(-1, n_timesteps, n_features)\n",
        "\n",
        "X_test_standardized = X_test.reshape(-1, n_timesteps * n_features)\n",
        "X_test_standardized = scaler.transform(X_test_standardized)\n",
        "X_test_standardized = X_test_standardized.reshape(-1, n_timesteps, n_features)\n",
        "\n",
        "# Reshape it back to the LSTM input shape\n"
      ],
      "metadata": {
        "id": "I1apbuY5OyX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pbounds2 = {\n",
        "    'neurons_layer1': (10, 100),   # Number of neurons in the first LSTM layer\n",
        "    'neurons_layer2': (5, 50),     # Number of neurons in the second LSTM layer\n",
        "    'neurons_layer3': (5, 50),     # Number of neurons in the third LSTM layer\n",
        "    'dropout_rate': (0.1, 0.5),    # Dropout rate for regularization\n",
        "}\n",
        "def gru_cv(neurons_layer1, neurons_layer2, neuron_layer3, dropout_rate):\n",
        "    model = build_model(neurons_layer1, neurons_layer2,neuron_layer3, dropout_rate)\n",
        "    # Assuming you have X_train, y_train as the training data\n",
        "    X_train_cv, X_val, y_train_cv, y_val = train_test_split(X_train_standardized, y_train, test_size=0.2, random_state=42)\n",
        "    model.fit(X_train_cv, y_train_cv, epochs=10, batch_size=1500, verbose=0)\n",
        "#    val_loss = model.evaluate(X_test_new, y_test_new, verbose=0)[0]\n",
        "    val_loss = model.evaluate(X_val, y_val, verbose=0)[0]\n",
        "    return -val_loss  # Minimize the loss (negative of validation loss)\n",
        "optimizer = BayesianOptimization(f=gru_cv, pbounds=pbounds2, random_state=42)\n"
      ],
      "metadata": {
        "id": "PHeOVwhGARZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#####\n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adagrad,Adam, Nadam, Adadelta, RMSprop, SGD\n",
        "from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score\n",
        "from bayes_opt import BayesianOptimization\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "\n",
        "from keras import backend as K\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "#import tensorflow as tf\n",
        "#from keras import backend as K\n",
        "\n",
        "def custom_binary_cross_entropy(y_true, y_pred, false_negative_weight=10.0):\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())  # Clip y_pred to avoid log(0) and log(1) issues\n",
        "\n",
        "    # Calculate binary cross-entropy loss\n",
        "    #loge = K.log(1-y_pred)\n",
        "    log10 = K.log(1-y_pred)/2.302585\n",
        "    #divide = tf.math.divide()\n",
        "    loss = -(y_true * K.log(y_pred) + (1 - y_true) * K.log(1 - y_pred)/2.302585)\n",
        "\n",
        "    # Apply weight for cases where y_true > y_pred (false negatives)\n",
        "    false_negative_mask = K.cast(K.greater(y_true, y_pred), K.floatx())  # 1 where y_true > y_pred, 0 otherwise\n",
        "    loss = loss - false_negative_mask * false_negative_weight * y_true * K.log(y_pred)\n",
        "\n",
        "    return K.mean(loss, axis=-1)  # Take the mean over the last axis (batch axis)\n",
        "\n",
        "def weighted_binary_cross_entropy(y_true, y_pred,one_weight = one_weight, zero_weight = zero_weight):\n",
        "\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())  # Clip y_pred to avoid log(0) and log(1) issues\n",
        "\n",
        "    loss = -(y_true * K.log(y_pred) * one_weight + (1 - y_true) * K.log(1 - y_pred) * zero_weight)#/2.302585) for ln(10), or 0.69314718056 for ln(2)\n",
        "    return K.mean(loss, axis = -1)\n",
        "\n",
        "pbounds2 = {\n",
        "    'neurons_layer1': (10, 500),   # Number of neurons in the first LSTM layer\n",
        "    'neurons_layer2': (5, 350),     # Number of neurons in the second LSTM layer\n",
        "    'neurons_layer3': (5, 250),     # Number of neurons in the third LSTM layer\n",
        "    'neurons_layer4': (5, 150),     # Number of neurons in the third LSTM layer\n",
        "    'neurons_layer5': (5, 100),     # Number of neurons in the third LSTM layer\n",
        "    'dropout_rate': (0.1, 0.5),    # Dropout rate for regularization\n",
        "}\n",
        "\n",
        "def build_model(neurons_layer1,neurons_layer2,neurons_layer3,neurons_layer4,neurons_layer5,dropout_rate):\n",
        "    model = Sequential()\n",
        "    model.add(GRU(int(neurons_layer1), input_shape=(1,30), dropout=float(dropout_rate), activation = 'relu', return_sequences = True))  # Input shape is (time_steps, num_features)\n",
        "#    model.add(BatchNormalization())\n",
        "    model.add(GRU(int(neurons_layer2),  return_sequences = True, activation = 'relu'))\n",
        "#    model.add(BatchNormalization())\n",
        "    model.add(GRU(int(neurons_layer3),return_sequences = True, activation = 'relu'))\n",
        "    model.add(GRU(int(neurons_layer4),return_sequences = True, activation = 'relu'))\n",
        "    model.add(GRU(int(neurons_layer5),return_sequences = True, activation = 'relu'))\n",
        "#    model.add(BatchNormalization())\n",
        "    model.add(Dense(1,activation = 'sigmoid'))\n",
        "    model.compile(loss=BinaryCrossentropy(from_logits = True), optimizer=Nadam(learning_rate=0.001), metrics=['accuracy'])#, Precision(), Recall()])\n",
        "    return model\n",
        "# Bayesian Optimization function\n",
        "\n",
        "def gru_cv(neurons_layer1, neurons_layer2, neurons_layer3,neurons_layer4,neurons_layer5, dropout_rate):\n",
        "    model = build_model(neurons_layer1, neurons_layer2,neurons_layer3,neurons_layer4,neurons_layer5, dropout_rate)\n",
        "    # Assuming you have X_train, y_train as the training data\n",
        "    X_train_cv, X_val, y_train_cv, y_val = train_test_split(X_train_standardized, y_train_new, test_size=0.2, random_state=42)\n",
        "    model.fit(X_train_cv, y_train_cv, epochs=10, batch_size=1500, verbose=0)\n",
        "#    val_loss = model.evaluate(X_test_new, y_test_new, verbose=0)[0]\n",
        "    val_loss = model.evaluate(X_val, y_val, verbose=0)[0]\n",
        "    return -val_loss  # Minimize the loss (negative of validation loss)\n",
        "# Perform Bayesian Optimization\n",
        "optimizer = BayesianOptimization(f=gru_cv, pbounds=pbounds2, random_state=42)\n",
        "optimizer.maximize(init_points=5, n_iter=10)\n",
        "\n",
        "best_params = optimizer.max['params']\n",
        "best_neuron_layer1 = int(best_params['neurons_layer1'])\n",
        "best_neuron_layer2 = int(best_params['neurons_layer2'])\n",
        "best_neuron_layer3 = int(best_params['neurons_layer3'])\n",
        "best_neuron_layer4 = int(best_params['neurons_layer4'])\n",
        "best_neuron_layer5 = int(best_params['neurons_layer5'])\n",
        "best_dropout_rate = best_params['dropout_rate']\n",
        "print(best_params,best_neuron_layer1,best_neuron_layer2,best_neuron_layer3,best_neuron_layer4,best_neuron_layer5,best_dropout_rate)\n",
        "model_gru = build_model(int(best_neuron_layer1),int(best_neuron_layer2),int(best_neuron_layer3),int(best_neuron_layer4),int(best_neuron_layer5),float(best_dropout_rate))"
      ],
      "metadata": {
        "id": "Ci6ybFwVyrC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_gru = build_model(int(best_neuron_layer1),int(best_neuron_layer2),int(best_neuron_layer3),float(best_dropout_rate))"
      ],
      "metadata": {
        "id": "CO6mwT-06ztF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import *\n",
        "from keras.optimizers import Adagrad,Adam, Nadam, Adadelta, RMSprop, SGD\n",
        "from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score\n",
        "from bayes_opt import BayesianOptimization\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from keras.regularizers import l1,l2\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from keras import backend as K\n",
        "from sklearn.utils import class_weight\n",
        "def build_model_2(neuron_layer1, neuron_layer2, neuron_layer3, neuron_layer4, neuron_layer5,\n",
        "                  dropout1 = 0.0, dropout2 = 0.0, dropout3 = 0.0, dropout4 = 0.0, regularization = None):\n",
        "    model2 = Sequential([\n",
        "        InputLayer(input_shape=(10, 1)),\n",
        "        Conv1D(int(neuron_layer1), kernel_size=3, activation='relu', kernel_regularizer=regularization),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2, padding='valid'),\n",
        "        Dropout(dropout1),\n",
        "        Conv1D(int(neuron_layer2), kernel_size=3, activation='relu', kernel_regularizer=regularization),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2, padding='valid'),\n",
        "        Dropout(dropout2),\n",
        "        GRU(int(neuron_layer3), return_sequences=True, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout3),\n",
        "        GRU(int(neuron_layer4), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout4),\n",
        "        Flatten(),\n",
        "        Dense(int(neuron_layer5), activation='relu', kernel_regularizer=regularization),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model2.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "    return model2\n",
        "\n",
        "pbounds3 = {\n",
        "    'neuron_layer1': (10,500),\n",
        "    'neuron_layer2': (10,500),\n",
        "    'neuron_layer3': (10,500),\n",
        "    'neuron_layer4': (10,500),\n",
        "    'neuron_layer5': (10,500),\n",
        "#    'dropout1': (0.05,0.5),\n",
        "#    'dropout2': (0.05,0.5),\n",
        "#    'dropout3': (0.05,0.5),\n",
        "#    'dropout4': (0.05,0.5),\n",
        "}\n",
        "X_train_new_reshaped = X_train_new.reshape((X_train_new.shape[0], 10, 1))\n",
        "\n",
        "def model_cv(neuron_layer1, neuron_layer2, neuron_layer3,neuron_layer4,neuron_layer5,  dropout1 = 0.2, dropout2 = 0.2, dropout3 = 0.2, dropout4 = 0.2):\n",
        "    model = build_model_2(neuron_layer1, neuron_layer2,neuron_layer3,neuron_layer4,neuron_layer5, dropout1, dropout2, dropout3, dropout4,None)\n",
        "    # Assuming you have X_train, y_train as the training data\n",
        "    X_train_cv, X_val, y_train_cv, y_val = train_test_split(X_train_new_reshaped, y_train_new, test_size=0.2, random_state=42)\n",
        "    model.fit(X_train_cv, y_train_cv, epochs=20, batch_size=8192, verbose=0)\n",
        "#    val_loss = model.evaluate(X_test_new, y_test_new, verbose=0)[0]\n",
        "    val_loss = model.evaluate(X_val, y_val, verbose=0)[0]\n",
        "    return -val_loss  # Minimize the loss (negative of validation loss)\n",
        "# Perform Bayesian Optimization\n",
        "optimizer = BayesianOptimization(f=model_cv, pbounds=pbounds3, random_state=42)\n",
        "optimizer.maximize(init_points=5, n_iter=10)\n",
        "\n",
        "best_params = optimizer.max['params']\n",
        "best_neuron_layer1 = int(best_params['neuron_layer1'])\n",
        "best_neuron_layer2 = int(best_params['neuron_layer2'])\n",
        "best_neuron_layer3 = int(best_params['neuron_layer3'])\n",
        "best_neuron_layer4 = int(best_params['neuron_layer4'])\n",
        "best_neuron_layer5 = int(best_params['neuron_layer5'])\n",
        "#best_dropout1 = best_params['dropout_rate']\n",
        "print(best_params,best_neuron_layer1,best_neuron_layer2,best_neuron_layer3,best_neuron_layer4,best_neuron_layer5)#,best_dropout_rate)\n",
        "model_2 = build_model_2(int(best_neuron_layer1),int(best_neuron_layer2),int(best_neuron_layer3),int(best_neuron_layer4),int(best_neuron_layer5),0.2,0.2,0.2,0.2)#,float(best_dropout_rate))\n",
        "#model2 = build_model_2()\n",
        "#model2.compile(loss = BinaryCrossentropy(from_logits = True), optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "#X_train_new_reshaped = X_train_standardized.reshape((X_train_new.shape[0], 30, 1))\n",
        "\n",
        "#X_train_cv, X_val, y_train_cv, y_val = train_test_split(X_train_new_reshaped, y_train_new, test_size=0.2, random_state=42)\n",
        "#es = EarlyStopping(monitor='val_loss', mode='min',patience=5)\n",
        "#model2.fit(X_train_cv, y_train_cv, epochs=20,batch_size=500,validation_data=(X_val,y_val),shuffle=False, callbacks=[es])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOBA7MS3rBw1",
        "outputId": "f926336d-e62b-4fa8-d941-962e245dcc23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_34 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_35 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | neuron... | neuron... | neuron... | neuron... | neuron... |\n",
            "-------------------------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_36 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_37 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[0m1        \u001b[0m | \u001b[0m-0.00317 \u001b[0m | \u001b[0m193.5    \u001b[0m | \u001b[0m475.9    \u001b[0m | \u001b[0m368.7    \u001b[0m | \u001b[0m303.3    \u001b[0m | \u001b[0m86.45    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_38 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_39 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[0m2        \u001b[0m | \u001b[0m-0.003895\u001b[0m | \u001b[0m86.44    \u001b[0m | \u001b[0m38.46    \u001b[0m | \u001b[0m434.4    \u001b[0m | \u001b[0m304.5    \u001b[0m | \u001b[0m357.0    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_40 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_41 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[95m3        \u001b[0m | \u001b[95m-0.002763\u001b[0m | \u001b[95m20.09    \u001b[0m | \u001b[95m485.3    \u001b[0m | \u001b[95m417.9    \u001b[0m | \u001b[95m114.0    \u001b[0m | \u001b[95m99.09    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_42 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_43 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[0m4        \u001b[0m | \u001b[0m-0.003026\u001b[0m | \u001b[0m99.87    \u001b[0m | \u001b[0m159.1    \u001b[0m | \u001b[0m267.1    \u001b[0m | \u001b[0m221.7    \u001b[0m | \u001b[0m152.7    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_44 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_45 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[0m5        \u001b[0m | \u001b[0m-0.003792\u001b[0m | \u001b[0m309.8    \u001b[0m | \u001b[0m78.35    \u001b[0m | \u001b[0m153.2    \u001b[0m | \u001b[0m189.5    \u001b[0m | \u001b[0m233.5    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_46 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_47 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[0m6        \u001b[0m | \u001b[0m-0.003361\u001b[0m | \u001b[0m199.7    \u001b[0m | \u001b[0m348.6    \u001b[0m | \u001b[0m136.4    \u001b[0m | \u001b[0m417.7    \u001b[0m | \u001b[0m308.8    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_48 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_49 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[0m7        \u001b[0m | \u001b[0m-0.004643\u001b[0m | \u001b[0m454.1    \u001b[0m | \u001b[0m61.62    \u001b[0m | \u001b[0m82.16    \u001b[0m | \u001b[0m463.4    \u001b[0m | \u001b[0m387.6    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_50 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_51 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[0m8        \u001b[0m | \u001b[0m-0.004112\u001b[0m | \u001b[0m296.2    \u001b[0m | \u001b[0m33.41    \u001b[0m | \u001b[0m27.55    \u001b[0m | \u001b[0m61.22    \u001b[0m | \u001b[0m124.3    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_52 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_53 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[0m9        \u001b[0m | \u001b[0m-0.002817\u001b[0m | \u001b[0m11.55    \u001b[0m | \u001b[0m471.7    \u001b[0m | \u001b[0m70.05    \u001b[0m | \u001b[0m127.6    \u001b[0m | \u001b[0m117.9    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_54 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_55 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[0m10       \u001b[0m | \u001b[0m-0.003999\u001b[0m | \u001b[0m405.7    \u001b[0m | \u001b[0m96.14    \u001b[0m | \u001b[0m165.2    \u001b[0m | \u001b[0m141.7    \u001b[0m | \u001b[0m365.7    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_56 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_57 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[0m11       \u001b[0m | \u001b[0m-0.003523\u001b[0m | \u001b[0m250.7    \u001b[0m | \u001b[0m135.3    \u001b[0m | \u001b[0m370.2    \u001b[0m | \u001b[0m301.9    \u001b[0m | \u001b[0m401.0    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_58 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_59 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[95m12       \u001b[0m | \u001b[95m-0.00274 \u001b[0m | \u001b[95m13.44    \u001b[0m | \u001b[95m461.2    \u001b[0m | \u001b[95m284.1    \u001b[0m | \u001b[95m63.16    \u001b[0m | \u001b[95m393.5    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_60 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_61 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[0m13       \u001b[0m | \u001b[0m-0.003075\u001b[0m | \u001b[0m297.5    \u001b[0m | \u001b[0m469.3    \u001b[0m | \u001b[0m474.6    \u001b[0m | \u001b[0m25.03    \u001b[0m | \u001b[0m420.5    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_62 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_63 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[0m14       \u001b[0m | \u001b[0m-0.004974\u001b[0m | \u001b[0m150.3    \u001b[0m | \u001b[0m315.5    \u001b[0m | \u001b[0m494.5    \u001b[0m | \u001b[0m10.34    \u001b[0m | \u001b[0m27.21    \u001b[0m |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_64 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_65 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| \u001b[95m15       \u001b[0m | \u001b[95m-0.002461\u001b[0m | \u001b[95m38.3     \u001b[0m | \u001b[95m403.5    \u001b[0m | \u001b[95m268.5    \u001b[0m | \u001b[95m238.2    \u001b[0m | \u001b[95m219.0    \u001b[0m |\n",
            "=====================================================================================\n",
            "{'neuron_layer1': 38.30307913971599, 'neuron_layer2': 403.4933364599485, 'neuron_layer3': 268.53245430461266, 'neuron_layer4': 238.1602431483159, 'neuron_layer5': 219.0295407766879} 38 403 268 238 219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_model(model):\n",
        "    neurons_in_layers = []\n",
        "    layer_types = []\n",
        "\n",
        "    for i, layer in enumerate(model_2.layers):\n",
        "        if isinstance(layer, Conv1D):\n",
        "            neurons = layer.filters\n",
        "            print(f\"Layer {i + 1} - Type: Conv1D (Filters: {neurons}), Filters: {neurons}\")\n",
        "        elif hasattr(layer, 'units'):\n",
        "            neurons = layer.units\n",
        "            print(f\"Layer {i + 1} - Type: {layer.__class__.__name__}, Neurons: {neurons}\")\n",
        "        elif isinstance(layer, Dropout):\n",
        "            rate = layer.rate\n",
        "            print(f\"Layer {i + 1} - Type: Dropout (Rate: {rate}), Dropout Rate: {rate}\")\n",
        "        elif isinstance(layer, Flatten):\n",
        "            print(f\"Layer {i + 1} - Type: Flatten\")\n",
        "        else:\n",
        "            print(f\"Layer {i + 1} - Type: {layer.__class__.__name__}\")\n"
      ],
      "metadata": {
        "id": "ZBWHmz0WubQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import *\n",
        "from keras.optimizers import Adagrad,Adam, Nadam, Adadelta, RMSprop, SGD\n",
        "from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score\n",
        "from bayes_opt import BayesianOptimization\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from keras.regularizers import l1,l2\n",
        "def build_model_2(neuron_layer1, neuron_layer2, neuron_layer3, neuron_layer4, neuron_layer5,\n",
        "                  dropout1 = 0.0, dropout2 = 0.0, dropout3 = 0.0, dropout4 = 0.0, regularization = None):\n",
        "    model2 = Sequential([\n",
        "        InputLayer(input_shape=(10, 1)),\n",
        "        Conv1D(int(neuron_layer1), kernel_size=3, activation='relu', kernel_regularizer=regularization),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2, padding='valid'),\n",
        "        Dropout(dropout1),\n",
        "        Conv1D(int(neuron_layer2), kernel_size=3, activation='relu', kernel_regularizer=regularization),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2, padding='valid'),\n",
        "        Dropout(dropout2),\n",
        "        GRU(int(neuron_layer3), return_sequences=True, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout3),\n",
        "        GRU(int(neuron_layer4), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout4),\n",
        "        Flatten(),\n",
        "        Dense(int(neuron_layer5), activation='relu', kernel_regularizer=regularization),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model2.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\n",
        "    return model2\n"
      ],
      "metadata": {
        "id": "AwwAi_sEx75D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import *\n",
        "from keras.optimizers import Adagrad,Adam, Nadam, Adadelta, RMSprop, SGD\n",
        "from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score\n",
        "from bayes_opt import BayesianOptimization\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from keras.regularizers import l1,l2\n",
        "\n",
        "def build_model_with_prelu(neuron_layer1, neuron_layer2, neuron_layer3, neuron_layer4, neuron_layer5,\n",
        "                           dropout1=0.0, dropout2=0.0, dropout3=0.0, dropout4=0.0, regularization=None):\n",
        "    model = Sequential([\n",
        "        InputLayer(input_shape=(10, 1)),\n",
        "        Conv1D(int(neuron_layer1), kernel_size=3, kernel_regularizer=regularization),\n",
        "        PReLU(),  # Use PReLU activation\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=1, padding='valid'),\n",
        "        Dropout(dropout1),\n",
        "        Conv1D(int(neuron_layer2), kernel_size=3, kernel_regularizer=regularization),\n",
        "        PReLU(),  # Use PReLU activation\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=1, padding='valid'),\n",
        "        Dropout(dropout2),\n",
        "        GRU(int(neuron_layer3), return_sequences=True),\n",
        "        PReLU(),  # Use PReLU activation\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout3),\n",
        "        GRU(int(neuron_layer4)),\n",
        "        PReLU(),  # Use PReLU activation\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout4),\n",
        "        Flatten(),\n",
        "        Dense(int(neuron_layer5), kernel_regularizer=regularization),\n",
        "        PReLU(),  # Use PReLU activation\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "iJcOjuGBQraO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.regularizers import l1,l2\n",
        "#model_2 = build_model_2(int(best_neuron_layer1),int(best_neuron_layer2),int(best_neuron_layer3),int(best_neuron_layer4),int(best_neuron_layer5),0.2,0.2,0.2,0.2) #regularization = l1(0.001))#,float(best_dropout_rate))\n",
        "#model_prelu = build_model_with_prelu(int(best_neuron_layer1),int(best_neuron_layer2),int(best_neuron_layer3),int(best_neuron_layer4),int(best_neuron_layer5),0.2,0.2,0.2,0.2) #regularization = l1(0.001))#,float(best_dropout_rate))\n",
        "\n",
        "model_2 = build_model_2(38, 403, 268, 238, 219, 0.2, 0.2, 0.2, 0.2,regularization = None)\n",
        "X_test_new_reshaped = X_test_new.reshape((X_test.shape[0], 10, 1))\n",
        "X_train_new_reshaped = X_train_new.reshape((X_train_new.shape[0], 10, 1))\n",
        "\n",
        "from sklearn.metrics import average_precision_score,precision_recall_curve,auc\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "es = EarlyStopping(monitor='val_loss', mode='min',patience=5)\n",
        "X_train_cv, X_val, y_train_cv, y_val = train_test_split(X_train_new_reshaped, y_train_new, test_size=0.2, random_state=42)\n",
        "#history_lstm = model_gru.fit(X_train_standardized, y_train_new, epochs=10,batch_size=500,validation_data=(X_test_standardized,y_test_new),shuffle=False, callbacks=[es])\n",
        "model_2.fit(X_train_cv, y_train_cv, epochs=30,batch_size=8192,validation_data=(X_val,y_val),shuffle=False, callbacks=[es])\n",
        "end = time.time()\n",
        "duration = end - start\n",
        "minutes, seconds = divmod(duration, 60)\n",
        "loss, accuracy = model_2.evaluate(X_test_new_reshaped, y_test_new)\n",
        "y_pred = model_2.predict(X_test_new_reshaped)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6luYCI2rEXk",
        "outputId": "01dea936-fd8d-44b6-cf11-06a34cc02677"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer gru_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer gru_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "20/20 [==============================] - 9s 104ms/step - loss: 0.1343 - accuracy: 0.9539 - val_loss: 0.4123 - val_accuracy: 0.9983\n",
            "Epoch 2/30\n",
            "20/20 [==============================] - 2s 78ms/step - loss: 0.0048 - accuracy: 0.9991 - val_loss: 0.1686 - val_accuracy: 0.9983\n",
            "Epoch 3/30\n",
            "20/20 [==============================] - 2s 80ms/step - loss: 0.0044 - accuracy: 0.9991 - val_loss: 0.0675 - val_accuracy: 0.9982\n",
            "Epoch 4/30\n",
            "20/20 [==============================] - 2s 79ms/step - loss: 0.0041 - accuracy: 0.9991 - val_loss: 0.0307 - val_accuracy: 0.9983\n",
            "Epoch 5/30\n",
            "20/20 [==============================] - 1s 72ms/step - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.0170 - val_accuracy: 0.9983\n",
            "Epoch 6/30\n",
            "20/20 [==============================] - 2s 76ms/step - loss: 0.0039 - accuracy: 0.9992 - val_loss: 0.0112 - val_accuracy: 0.9983\n",
            "Epoch 7/30\n",
            "20/20 [==============================] - 1s 72ms/step - loss: 0.0037 - accuracy: 0.9992 - val_loss: 0.0085 - val_accuracy: 0.9984\n",
            "Epoch 8/30\n",
            "20/20 [==============================] - 1s 73ms/step - loss: 0.0036 - accuracy: 0.9993 - val_loss: 0.0073 - val_accuracy: 0.9984\n",
            "Epoch 9/30\n",
            "20/20 [==============================] - 1s 73ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.0066 - val_accuracy: 0.9985\n",
            "Epoch 10/30\n",
            "20/20 [==============================] - 1s 73ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0060 - val_accuracy: 0.9986\n",
            "Epoch 11/30\n",
            "20/20 [==============================] - 2s 78ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0052 - val_accuracy: 0.9987\n",
            "Epoch 12/30\n",
            "20/20 [==============================] - 2s 80ms/step - loss: 0.0034 - accuracy: 0.9993 - val_loss: 0.0049 - val_accuracy: 0.9988\n",
            "Epoch 13/30\n",
            "20/20 [==============================] - 2s 80ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0041 - val_accuracy: 0.9992\n",
            "Epoch 14/30\n",
            "20/20 [==============================] - 1s 73ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0040 - val_accuracy: 0.9992\n",
            "Epoch 15/30\n",
            "20/20 [==============================] - 2s 77ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0038 - val_accuracy: 0.9993\n",
            "Epoch 16/30\n",
            "20/20 [==============================] - 2s 77ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0033 - val_accuracy: 0.9994\n",
            "Epoch 17/30\n",
            "20/20 [==============================] - 1s 73ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0032 - val_accuracy: 0.9994\n",
            "Epoch 18/30\n",
            "20/20 [==============================] - 1s 73ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0029 - val_accuracy: 0.9994\n",
            "Epoch 19/30\n",
            "20/20 [==============================] - 2s 77ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0028 - val_accuracy: 0.9995\n",
            "Epoch 20/30\n",
            "20/20 [==============================] - 2s 79ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0028 - val_accuracy: 0.9995\n",
            "Epoch 21/30\n",
            "20/20 [==============================] - 2s 81ms/step - loss: 0.0030 - accuracy: 0.9993 - val_loss: 0.0027 - val_accuracy: 0.9996\n",
            "Epoch 22/30\n",
            "20/20 [==============================] - 2s 79ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0027 - val_accuracy: 0.9996\n",
            "Epoch 23/30\n",
            "20/20 [==============================] - 2s 77ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0028 - val_accuracy: 0.9995\n",
            "Epoch 24/30\n",
            "20/20 [==============================] - 1s 73ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0027 - val_accuracy: 0.9996\n",
            "Epoch 25/30\n",
            "20/20 [==============================] - 2s 78ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
            "Epoch 26/30\n",
            "20/20 [==============================] - 1s 73ms/step - loss: 0.0027 - accuracy: 0.9993 - val_loss: 0.0027 - val_accuracy: 0.9996\n",
            "Epoch 27/30\n",
            "20/20 [==============================] - 2s 78ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
            "Epoch 28/30\n",
            "20/20 [==============================] - 2s 78ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0027 - val_accuracy: 0.9996\n",
            "Epoch 29/30\n",
            "20/20 [==============================] - 2s 82ms/step - loss: 0.0027 - accuracy: 0.9994 - val_loss: 0.0027 - val_accuracy: 0.9996\n",
            "2660/2660 [==============================] - 11s 4ms/step - loss: 0.0032 - accuracy: 0.9995\n",
            "2660/2660 [==============================] - 9s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_model(model_2)\n",
        "t = 0.5\n",
        "y_pred_binary = (y_pred > t).astype(int)  # Convert probabilities to binary predictions\n",
        "print(\"Threshold value is \" + str(t))\n",
        "print(y_pred.shape)\n",
        "y_new_test= y_test_new.reshape(-1)  # or y_test_new = np.squeeze(y_test_new)\n",
        "\n",
        "y_pred_binary = np.squeeze(y_pred_binary)\n",
        "# Calculate evaluation metrics\n",
        "precision = precision_score(y_new_test, y_pred_binary)\n",
        "recall = recall_score(y_new_test, y_pred_binary)\n",
        "f1 = f1_score(y_new_test, y_pred_binary)\n",
        "roc_auc = roc_auc_score(y_new_test, y_pred_binary)\n",
        "mcc = matthews_corrcoef(y_new_test, y_pred_binary)\n",
        "\n",
        "# Calculate the area under the precision-recall curve\n",
        "#auc_pr = average_precision_score(y_new_test, y_pred_binary)\n",
        "precision2, recall2, thresholds = precision_recall_curve(y_new_test, y_pred_binary)\n",
        "pr_auc = auc(recall2, precision2)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test F1-score: {f1:.4f}\")\n",
        "print(f\"Test AUC-ROC: {roc_auc:.4f}\")\n",
        "print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
        "\n",
        "# Print the AUC-PR\n",
        "#print(f\"Test AUC-PR: {auc_pr:.4f}\")\n",
        "print(f\"AUC-PR : {pr_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6sbHXEle8FK",
        "outputId": "2454de68-acbb-4be8-8ae7-644c4568cb1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1 - Type: Conv1D (Filters: 38), Filters: 38\n",
            "Layer 2 - Type: BatchNormalization\n",
            "Layer 3 - Type: MaxPooling1D\n",
            "Layer 4 - Type: Dropout (Rate: 0.2), Dropout Rate: 0.2\n",
            "Layer 5 - Type: Conv1D (Filters: 403), Filters: 403\n",
            "Layer 6 - Type: BatchNormalization\n",
            "Layer 7 - Type: MaxPooling1D\n",
            "Layer 8 - Type: Dropout (Rate: 0.2), Dropout Rate: 0.2\n",
            "Layer 9 - Type: GRU, Neurons: 268\n",
            "Layer 10 - Type: BatchNormalization\n",
            "Layer 11 - Type: Dropout (Rate: 0.2), Dropout Rate: 0.2\n",
            "Layer 12 - Type: GRU, Neurons: 238\n",
            "Layer 13 - Type: BatchNormalization\n",
            "Layer 14 - Type: Dropout (Rate: 0.2), Dropout Rate: 0.2\n",
            "Layer 15 - Type: Flatten\n",
            "Layer 16 - Type: Dense, Neurons: 219\n",
            "Layer 17 - Type: Dense, Neurons: 1\n",
            "Threshold value is 0.5\n",
            "(85118, 1)\n",
            "Test Loss: 0.0032\n",
            "Test Accuracy: 0.9995\n",
            "Test Precision: 0.8870\n",
            "Test Recall: 0.7612\n",
            "Test F1-score: 0.8193\n",
            "Test AUC-ROC: 0.8805\n",
            "Matthews Correlation Coefficient: 0.8214\n",
            "AUC-PR : 0.8243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.resetwarnings()\n",
        "import numpy as np\n",
        "import optuna\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import InputLayer, Conv1D, BatchNormalization, MaxPooling1D, Dropout, GRU, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from keras.regularizers import l1, l2\n",
        "\n",
        "def build_model_2(neuron_layer1, neuron_layer2, neuron_layer3, neuron_layer4, neuron_layer5,\n",
        "                  dropout1=0.0, dropout2=0.0, dropout3=0.0, dropout4=0.0, regularization=None):\n",
        "    model = Sequential([\n",
        "        InputLayer(input_shape=(30, 1)),\n",
        "        Conv1D(int(neuron_layer1), kernel_size=3, activation='relu', kernel_regularizer=regularization),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2, padding='valid'),\n",
        "        Dropout(dropout1),\n",
        "        Conv1D(int(neuron_layer2), kernel_size=3, activation='relu', kernel_regularizer=regularization),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(pool_size=2, padding='valid'),\n",
        "        Dropout(dropout2),\n",
        "        GRU(int(neuron_layer3), return_sequences=True, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout3),\n",
        "        GRU(int(neuron_layer4), activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(dropout4),\n",
        "        Flatten(),\n",
        "        Dense(int(neuron_layer5), activation='relu', kernel_regularizer=regularization),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "X_train_new_reshaped = X_train_new.reshape((X_train_new.shape[0], 30, 1))\n",
        "\n",
        "def objective(trial):\n",
        "    neuron_layer1 = trial.suggest_int('neuron_layer1', 10, 500)\n",
        "    neuron_layer2 = trial.suggest_int('neuron_layer2', 10, 500)\n",
        "    neuron_layer3 = trial.suggest_int('neuron_layer3', 10, 500)\n",
        "    neuron_layer4 = trial.suggest_int('neuron_layer4', 10, 500)\n",
        "    neuron_layer5 = trial.suggest_int('neuron_layer5', 10, 500)\n",
        "    dropout1 = trial.suggest_float('dropout1', 0.05, 0.5)\n",
        "    dropout2 = trial.suggest_float('dropout2', 0.05, 0.5)\n",
        "    dropout3 = trial.suggest_float('dropout3', 0.05, 0.5)\n",
        "    dropout4 = trial.suggest_float('dropout4', 0.05, 0.5)\n",
        "    regularization = l1(0.005)\n",
        "\n",
        "    model = build_model_2(neuron_layer1, neuron_layer2, neuron_layer3, neuron_layer4, neuron_layer5,\n",
        "                          dropout1, dropout2, dropout3, dropout4, regularization)\n",
        "\n",
        "    X_train_cv, X_val, y_train_cv, y_val = train_test_split(X_train_new_reshaped, y_train_new, test_size=0.2, random_state=42)\n",
        "    model.fit(X_train_cv, y_train_cv, epochs=10, batch_size=4096, verbose=0)\n",
        "    val_loss = model.evaluate(X_val, y_val, verbose=0)[0]\n",
        "    return val_loss\n",
        "\n",
        "# Create a study object and optimize\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=100)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = study.best_params\n",
        "best_neuron_layer1 = best_params['neuron_layer1']\n",
        "best_neuron_layer2 = best_params['neuron_layer2']\n",
        "best_neuron_layer3 = best_params['neuron_layer3']\n",
        "best_neuron_layer4 = best_params['neuron_layer4']\n",
        "best_neuron_layer5 = best_params['neuron_layer5']\n",
        "best_dropout1 = best_params['dropout1']\n",
        "best_dropout2 = best_params['dropout2']\n",
        "best_dropout3 = best_params['dropout3']\n",
        "best_dropout4 = best_params['dropout4']\n",
        "\n",
        "# Build the final model with the best hyperparameters\n",
        "final_model = build_model_2(best_neuron_layer1, best_neuron_layer2, best_neuron_layer3, best_neuron_layer4, best_neuron_layer5,\n",
        "                            best_dropout1, best_dropout2, best_dropout3, best_dropout4, l1(0.005))\n",
        "\n",
        "# Train the final model on the full training set\n",
        "final_model.fit(X_train_new_reshaped, y_train_new, epochs=10, batch_size=4096, verbose=0)\n",
        "\n",
        "# Evaluate the final model\n",
        "final_loss, final_accuracy = final_model.evaluate(X_val, y_val, verbose=0)\n",
        "print(\"Final Validation Loss:\", final_loss)\n",
        "print(\"Final Validation Accuracy:\", final_accuracy)\n"
      ],
      "metadata": {
        "id": "zIFj6YBYVQJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import *\n",
        "from keras.optimizers import Adagrad,Adam, Nadam, Adadelta, RMSprop, SGD\n",
        "from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score\n",
        "from bayes_opt import BayesianOptimization\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "\n",
        "from keras import backend as K\n",
        "from sklearn.utils import class_weight\n",
        "def build_model_2(neuron_layer1, neuron_layer2, neuron_layer3, neuron_layer4, neuron_layer5,\n",
        "                  dropout1=0.2, dropout2=0.2, dropout3=0.2, dropout4=0.2, regularization=None):\n",
        "    model2 = Sequential([\n",
        "        InputLayer(input_shape=(30, 1)),\n",
        "        Conv1D(int(neuron_layer1), kernel_size=3, activation='relu', kernel_regularizer=regularization),\n",
        "        MaxPooling1D(pool_size=2, padding='valid'),\n",
        "        Dropout(dropout1),\n",
        "        Conv1D(int(neuron_layer2), kernel_size=3, activation='relu', kernel_regularizer=regularization),\n",
        "        MaxPooling1D(pool_size=2, padding='valid'),\n",
        "        Dropout(dropout2),\n",
        "        GRU(int(neuron_layer3), return_sequences=True, activation='relu'),\n",
        "        Dropout(dropout3),\n",
        "        GRU(int(neuron_layer4), activation='relu'),\n",
        "        Dropout(dropout4),\n",
        "        Flatten(),\n",
        "        Dense(int(neuron_layer5), activation='relu', kernel_regularizer=regularization),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    return model2"
      ],
      "metadata": {
        "id": "yM8DpJ2db9m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_params = optimizer.max['params']\n",
        "best_neuron_layer1 = int(best_params['neuron_layer1'])\n",
        "best_neuron_layer2 = int(best_params['neuron_layer2'])\n",
        "best_neuron_layer3 = int(best_params['neuron_layer3'])\n",
        "best_neuron_layer4 = int(best_params['neuron_layer4'])\n",
        "best_neuron_layer5 = int(best_params['neuron_layer5'])\n",
        "#best_dropout1 = best_params['dropout_rate']\n",
        "print(best_params,best_neuron_layer1,best_neuron_layer2,best_neuron_layer3,best_neuron_layer4,best_neuron_layer5)#,best_dropout_rate)\n",
        "model_2 = build_model(int(best_neuron_layer1),int(best_neuron_layer2),int(best_neuron_layer3),int(best_neuron_layer4),int(best_neuron_layer5))#,float(best_dropout_rate))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "W94VX9o1bZoj",
        "outputId": "f147de58-65e2-4800-e7e0-757d8c8db09e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-b022df9ab071>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mbest_neuron_layer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neuron_layer1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbest_neuron_layer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neuron_layer2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbest_neuron_layer3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neuron_layer3'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbest_neuron_layer4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'neuron_layer4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'optimizer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_model(model_2)\n",
        "\n",
        "loss, accuracy = model_2.evaluate(X_test_new_reshaped, y_test_new)\n",
        "y_pred = model_2.predict(X_test_new_reshaped)\n",
        "y_pred_binary = (y_pred > 0.25).astype(int)  # Convert probabilities to binary predictions\n",
        "print(y_pred.shape)\n",
        "y_new_test= y_test_new.reshape(-1)  # or y_test_new = np.squeeze(y_test_new)\n",
        "\n",
        "y_pred_binary = np.squeeze(y_pred_binary)\n",
        "# Calculate evaluation metrics\n",
        "precision = precision_score(y_new_test, y_pred_binary)\n",
        "recall = recall_score(y_new_test, y_pred_binary)\n",
        "f1 = f1_score(y_new_test, y_pred_binary)\n",
        "roc_auc = roc_auc_score(y_new_test, y_pred_binary)\n",
        "mcc = matthews_corrcoef(y_new_test, y_pred_binary)\n",
        "\n",
        "# Calculate the area under the precision-recall curve\n",
        "auc_pr = average_precision_score(y_new_test, y_pred_binary)\n",
        "precision2, recall2, thresholds = precision_recall_curve(y_new_test, y_pred_binary)\n",
        "pr_auc = auc(recall2, precision2)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test F1-score: {f1:.4f}\")\n",
        "print(f\"Test AUC-ROC: {roc_auc:.4f}\")\n",
        "print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
        "\n",
        "# Print the AUC-PR\n",
        "print(f\"Test AUC-PR: {auc_pr:.4f}\")\n",
        "print(f\"AUC-PR : {pr_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lraFHIlHtzyv",
        "outputId": "16de3954-f6a7-4d60-8acf-000cd5659472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer 1 - Type: Conv1D (Filters: 128), Filters: 128\n",
            "Layer 2 - Type: MaxPooling1D\n",
            "Layer 3 - Type: Dropout (Rate: 0), Dropout Rate: 0\n",
            "Layer 4 - Type: Conv1D (Filters: 128), Filters: 128\n",
            "Layer 5 - Type: MaxPooling1D\n",
            "Layer 6 - Type: Dropout (Rate: 0), Dropout Rate: 0\n",
            "Layer 7 - Type: GRU, Neurons: 256\n",
            "Layer 8 - Type: Dropout (Rate: 0), Dropout Rate: 0\n",
            "Layer 9 - Type: GRU, Neurons: 32\n",
            "Layer 10 - Type: Dropout (Rate: 0), Dropout Rate: 0\n",
            "Layer 11 - Type: Flatten\n",
            "Layer 12 - Type: Dense, Neurons: 128\n",
            "Layer 13 - Type: Dense, Neurons: 1\n",
            "(85118, 1)\n",
            "Test Loss: 0.0041\n",
            "Test Accuracy: 0.9995\n",
            "Test Precision: 0.9505\n",
            "Test Recall: 0.7164\n",
            "Test F1-score: 0.8170\n",
            "Test AUC-ROC: 0.8582\n",
            "Matthews Correlation Coefficient: 0.8250\n",
            "Test AUC-PR: 0.6814\n",
            "AUC-PR : 0.8337\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_binary = (y_pred > 0.25).astype(int)  # Convert probabilities to binary predictions\n",
        "print(y_pred.shape)\n",
        "y_new_test= y_test_new.reshape(-1)  # or y_test_new = np.squeeze(y_test_new)\n",
        "\n",
        "y_pred_binary = np.squeeze(y_pred_binary)\n",
        "# Calculate evaluation metrics\n",
        "precision = precision_score(y_new_test, y_pred_binary)\n",
        "recall = recall_score(y_new_test, y_pred_binary)\n",
        "f1 = f1_score(y_new_test, y_pred_binary)\n",
        "roc_auc = roc_auc_score(y_new_test, y_pred_binary)\n",
        "mcc = matthews_corrcoef(y_new_test, y_pred_binary)\n",
        "\n",
        "# Calculate the area under the precision-recall curve\n",
        "auc_pr = average_precision_score(y_new_test, y_pred_binary)\n",
        "precision2, recall2, thresholds = precision_recall_curve(y_new_test, y_pred_binary)\n",
        "pr_auc = auc(recall2, precision2)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test F1-score: {f1:.4f}\")\n",
        "print(f\"Test AUC-ROC: {roc_auc:.4f}\")\n",
        "print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
        "\n",
        "# Print the AUC-PR\n",
        "print(f\"Test AUC-PR: {auc_pr:.4f}\")\n",
        "print(f\"AUC-PR : {pr_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hclaDGwkec3b",
        "outputId": "b36d43cd-c69d-46c6-8c51-ff2b588ad09a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(85118, 1)\n",
            "Test Loss: 0.0032\n",
            "Test Accuracy: 0.9995\n",
            "Test Precision: 0.8947\n",
            "Test Recall: 0.7612\n",
            "Test F1-score: 0.8226\n",
            "Test AUC-ROC: 0.8805\n",
            "Matthews Correlation Coefficient: 0.8250\n",
            "Test AUC-PR: 0.6814\n",
            "AUC-PR : 0.8282\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import average_precision_score,precision_recall_curve,auc\n",
        "import time\n",
        "start = time.time()\n",
        "es = EarlyStopping(monitor='val_loss', mode='min',patience=5)\n",
        "X_train_cv, X_val, y_train_cv, y_val = train_test_split(X_train_standardized, y_train_new, test_size=0.2, random_state=42)\n",
        "#history_lstm = model_gru.fit(X_train_standardized, y_train_new, epochs=10,batch_size=500,validation_data=(X_test_standardized,y_test_new),shuffle=False, callbacks=[es])\n",
        "model_gru.fit(X_train_cv, y_train_cv, epochs=20,batch_size=500,validation_data=(X_val,y_val),shuffle=False, callbacks=[es])\n",
        "end = time.time()\n",
        "duration = end - start\n",
        "minutes, seconds = divmod(duration, 60)\n",
        "\n",
        "model_gru.summary()\n",
        "\n",
        "print(f\"Training completed in {int(minutes)} minutes and {round(seconds, 2)} seconds.\")\n",
        "\n",
        "\n",
        "# Evaluate on the test set\n",
        "loss, accuracy = model_gru.evaluate(X_test_standardized, y_test_new)\n",
        "y_pred = model_gru.predict(X_test_standardized)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "print(y_pred.shape)\n",
        "y_new_test= y_test_new.reshape(-1)  # or y_test_new = np.squeeze(y_test_new)\n",
        "\n",
        "y_pred_binary = np.squeeze(y_pred_binary)\n",
        "# Calculate evaluation metrics\n",
        "precision = precision_score(y_new_test, y_pred_binary)\n",
        "recall = recall_score(y_new_test, y_pred_binary)\n",
        "f1 = f1_score(y_new_test, y_pred_binary)\n",
        "roc_auc = roc_auc_score(y_new_test, y_pred_binary)\n",
        "mcc = matthews_corrcoef(y_new_test, y_pred_binary)\n",
        "\n",
        "# Calculate the area under the precision-recall curve\n",
        "auc_pr = average_precision_score(y_new_test, y_pred_binary)\n",
        "precision2, recall2, thresholds = precision_recall_curve(y_new_test, y_pred_binary)\n",
        "pr_auc = auc(recall2, precision2)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test F1-score: {f1:.4f}\")\n",
        "print(f\"Test AUC-ROC: {roc_auc:.4f}\")\n",
        "print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
        "\n",
        "# Print the AUC-PR\n",
        "print(f\"Test AUC-PR: {auc_pr:.4f}\")\n",
        "print(f\"AUC-PR : {pr_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jR0gvcL_y9A9",
        "outputId": "52742548-e586-4ca6-af80-713622644057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "318/318 [==============================] - 22s 44ms/step - loss: 0.0384 - accuracy: 0.9968 - val_loss: 0.0036 - val_accuracy: 0.9995\n",
            "Epoch 2/20\n",
            "318/318 [==============================] - 13s 41ms/step - loss: 0.0039 - accuracy: 0.9993 - val_loss: 0.0031 - val_accuracy: 0.9995\n",
            "Epoch 3/20\n",
            "318/318 [==============================] - 13s 40ms/step - loss: 0.0033 - accuracy: 0.9993 - val_loss: 0.0030 - val_accuracy: 0.9995\n",
            "Epoch 4/20\n",
            "318/318 [==============================] - 12s 36ms/step - loss: 0.0032 - accuracy: 0.9993 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
            "Epoch 5/20\n",
            "318/318 [==============================] - 12s 36ms/step - loss: 0.0031 - accuracy: 0.9993 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
            "Epoch 6/20\n",
            "318/318 [==============================] - 13s 40ms/step - loss: 0.0029 - accuracy: 0.9994 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
            "Epoch 7/20\n",
            "318/318 [==============================] - 13s 40ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0025 - val_accuracy: 0.9994\n",
            "Epoch 8/20\n",
            "318/318 [==============================] - 12s 39ms/step - loss: 0.0028 - accuracy: 0.9993 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
            "Epoch 9/20\n",
            "318/318 [==============================] - 12s 39ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0025 - val_accuracy: 0.9994\n",
            "Epoch 10/20\n",
            "318/318 [==============================] - 12s 39ms/step - loss: 0.0026 - accuracy: 0.9993 - val_loss: 0.0026 - val_accuracy: 0.9994\n",
            "Epoch 11/20\n",
            "318/318 [==============================] - 11s 34ms/step - loss: 0.0029 - accuracy: 0.9993 - val_loss: 0.0024 - val_accuracy: 0.9994\n",
            "Epoch 12/20\n",
            "318/318 [==============================] - 12s 39ms/step - loss: 0.0025 - accuracy: 0.9994 - val_loss: 0.0025 - val_accuracy: 0.9994\n",
            "Epoch 13/20\n",
            "318/318 [==============================] - 13s 41ms/step - loss: 0.0024 - accuracy: 0.9993 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
            "Epoch 14/20\n",
            "318/318 [==============================] - 13s 40ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0026 - val_accuracy: 0.9995\n",
            "Epoch 15/20\n",
            "318/318 [==============================] - 13s 40ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0027 - val_accuracy: 0.9995\n",
            "Epoch 16/20\n",
            "318/318 [==============================] - 27s 85ms/step - loss: 0.0025 - accuracy: 0.9993 - val_loss: 0.0025 - val_accuracy: 0.9995\n",
            "Model: \"sequential_38\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " gru_187 (GRU)               (None, 1, 496)            785664    \n",
            "                                                                 \n",
            " gru_188 (GRU)               (None, 1, 78)             134784    \n",
            "                                                                 \n",
            " gru_189 (GRU)               (None, 1, 176)            135168    \n",
            "                                                                 \n",
            " gru_190 (GRU)               (None, 1, 76)             57912     \n",
            "                                                                 \n",
            " gru_191 (GRU)               (None, 1, 32)             10560     \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 1, 1)              33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,124,121\n",
            "Trainable params: 1,124,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Training completed in 3 minutes and 42.32 seconds.\n",
            "2660/2660 [==============================] - 21s 8ms/step - loss: 0.0033 - accuracy: 0.9994\n",
            "2660/2660 [==============================] - 13s 5ms/step\n",
            "(85118, 1, 1)\n",
            "Test Loss: 0.0033\n",
            "Test Accuracy: 0.9994\n",
            "Test Precision: 0.9118\n",
            "Test Recall: 0.6940\n",
            "Test F1-score: 0.7881\n",
            "Test AUC-ROC: 0.8470\n",
            "Matthews Correlation Coefficient: 0.7952\n",
            "Test AUC-PR: 0.6333\n",
            "AUC-PR : 0.8031\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_new_test,y_pred_binary))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_4Z_hleKEDgy",
        "outputId": "b097e1c5-beb8-47bc-feb9-66dfa6ec10e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     84984\n",
            "           1       0.89      0.69      0.78       134\n",
            "\n",
            "    accuracy                           1.00     85118\n",
            "   macro avg       0.95      0.84      0.89     85118\n",
            "weighted avg       1.00      1.00      1.00     85118\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_new_test.shape, y_pred_binary.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lACxMCvMHFlW",
        "outputId": "fbec9c54-d224-4059-ebc5-9942c889bbe5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(85118,) (85118, 1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_binary = np.squeeze(y_pred_binary)\n",
        "precision = precision_score(y_new_test, y_pred_binary)\n",
        "recall = recall_score(y_new_test, y_pred_binary)\n",
        "f1 = f1_score(y_new_test, y_pred_binary)\n",
        "roc_auc = roc_auc_score(y_new_test, y_pred_binary)\n",
        "mcc = matthews_corrcoef(y_new_test, y_pred_binary)\n",
        "\n",
        "# Calculate the area under the precision-recall curve\n",
        "auc_pr = average_precision_score(y_new_test, y_pred_binary)\n",
        "precision2, recall2, thresholds = precision_recall_curve(y_new_test, y_pred_binary)\n",
        "pr_auc = auc(recall2, precision2)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test F1-score: {f1:.4f}\")\n",
        "print(f\"Test AUC-ROC: {roc_auc:.4f}\")\n",
        "print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
        "\n",
        "# Print the AUC-PR\n",
        "print(f\"Test AUC-PR: {auc_pr:.4f}\")\n",
        "print(f\"AUC-PR : {pr_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLL2yrmkGw9m",
        "outputId": "ff794f8a-2d36-45c7-a270-8fd0ce1b7b58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0029\n",
            "Test Accuracy: 0.9995\n",
            "Test Precision: 0.9107\n",
            "Test Recall: 0.7612\n",
            "Test F1-score: 0.8293\n",
            "Test AUC-ROC: 0.8805\n",
            "Matthews Correlation Coefficient: 0.8324\n",
            "Test AUC-PR: 0.6936\n",
            "AUC-PR : 0.8361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_threshold(threshold,y_pred = y_pred,y_test_new = y_test_new):\n",
        "    y_pred_binary = (y_pred > threshold).astype(int)  # Convert probabilities to binary predictions\n",
        "    print(threshold)\n",
        "    y_new_test= y_test_new.reshape(-1)  # or y_test_new = np.squeeze(y_test_new)\n",
        "\n",
        "    # Calculate evaluation metrics\n",
        "    precision = precision_score(y_new_test, y_pred_binary)\n",
        "    recall = recall_score(y_new_test, y_pred_binary)\n",
        "    f1 = f1_score(y_new_test, y_pred_binary)\n",
        "    roc_auc = roc_auc_score(y_new_test, y_pred)\n",
        "    mcc = matthews_corrcoef(y_new_test, y_pred_binary)\n",
        "\n",
        "    # Calculate the area under the precision-recall curve\n",
        "    auc_pr = average_precision_score(y_new_test, y_pred)\n",
        "    precision2, recall2, thresholds = precision_recall_curve(y_new_test, y_pred)\n",
        "    pr_auc = auc(recall2, precision2)\n",
        "\n",
        "    # Print the metrics\n",
        "    print(f\"Test Loss: {loss:.4f}\")\n",
        "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Test Precision: {precision:.4f}\")\n",
        "    print(f\"Test Recall: {recall:.4f}\")\n",
        "    print(f\"Test F1-score: {f1:.4f}\")\n",
        "    print(f\"Test AUC-ROC: {roc_auc:.4f}\")\n",
        "    print(f\"Matthews Correlation Coefficient: {mcc:.4f}\")\n",
        "\n",
        "    # Print the AUC-PR\n",
        "    print(f\"Test AUC-PR: {auc_pr:.4f}\")\n",
        "    print(f\"AUC-PR : {pr_auc:.4f}\")\n",
        "    return mcc"
      ],
      "metadata": {
        "id": "aXoP6Ovv756E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxi = 0\n",
        "j = 0.1\n",
        "new = 0\n",
        "for i in np.arange(0.1,0.9,0.1):\n",
        "    new = print_threshold(i)\n",
        "    if new > maxi:\n",
        "        maxi = new\n",
        "        j = i\n",
        "print(j, \"   \" , maxi)\n",
        "print_threshold(j)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "mhDiA4yo8KJr",
        "outputId": "d87ac51b-6323-45ba-84b8-e6bef0543916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-016bfb052b00>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprint_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmaxi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mmaxi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-92-a38a54f8c1a5>\u001b[0m in \u001b[0;36mprint_threshold\u001b[0;34m(threshold, y_pred, y_test_new)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Calculate evaluation metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_new_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_new_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_new_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1952\u001b[0m     \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1953\u001b[0m     \"\"\"\n\u001b[0;32m-> 1954\u001b[0;31m     p, _, _, _ = precision_recall_fscore_support(\n\u001b[0m\u001b[1;32m   1955\u001b[0m         \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1956\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1571\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1572\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1573\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_set_wise_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1575\u001b[0m     \u001b[0;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"average has to be one of \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m     \u001b[0;31m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m     \u001b[0;31m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m     96\u001b[0m             \"Classification metrics can't handle a mix of {0} and {1} targets\".format(\n\u001b[1;32m     97\u001b[0m                 \u001b[0mtype_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype_pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and unknown targets"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(maxi,j)\n",
        "print_threshold(0.1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N0fw0lJ49FwM",
        "outputId": "b5fa2d71-ab01-423f-a322-18e34d6836d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7446593050333917 0.1\n",
            "0.1\n",
            "Test Loss: 0.0062\n",
            "Test Accuracy: 0.9994\n",
            "Test Precision: 0.8430\n",
            "Test Recall: 0.7612\n",
            "Test F1-score: 0.8000\n",
            "Test AUC-ROC: 0.7974\n",
            "Matthews Correlation Coefficient: 0.8007\n",
            "Test AUC-PR: 0.7447\n",
            "AUC-PR : 0.7445\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7446593050333917"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate evaluation metrics\n",
        "precision = precision_score(y_new_test, y_pred_binary)\n",
        "recall = recall_score(y_new_test, y_pred_binary)\n",
        "f1 = f1_score(y_new_test, y_pred_binary)\n",
        "roc_auc = roc_auc_score(y_new_test, y_pred)\n",
        "mcc = matthews_corrcoef(y_new_test, y_pred_binary)\n",
        "\n",
        "# Calculate the area under the precision-recall curve\n",
        "auc_pr = average_precision_score(y_new_test, y_pred)\n",
        "precision2, recall2, thresholds = precision_recall_curve(y_new_test, y_pred)\n",
        "pr_auc = auc(recall2, precision2)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test F1-score: {f1:.4f}\")\n",
        "print(f\"Test AUC-ROC: {roc_auc:.4f}\")\n",
        "print(\"Matthews Correlation Coefficient:\", mcc)\n",
        "\n",
        "# Print the AUC-PR\n",
        "print(f\"Test AUC-PR: {auc_pr:.4f}\")\n",
        "print(f\"AUC-PR : {pr_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFie3gE81EK1",
        "outputId": "8d321d57-3385-4f19-b09e-3beee9ce1867"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0056\n",
            "Test Accuracy: 0.9994\n",
            "Test Precision: 0.9293\n",
            "Test Recall: 0.6866\n",
            "Test F1-score: 0.7897\n",
            "Test AUC-ROC: 0.9162\n",
            "Matthews Correlation Coefficient: 0.7985025599803738\n",
            "Test AUC-PR: 0.7793\n",
            "AUC-PR : 0.7791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score\n",
        "from bayes_opt import BayesianOptimization\n",
        "from keras import backend as K\n",
        "\n",
        "pbounds = {\n",
        "    'gen_neurons_layer1': (10, 100),   # Number of neurons in the first generator layer\n",
        "    'gen_neurons_layer2': (5, 50),     # Number of neurons in the second generator layer\n",
        "    'disc_neurons_layer1': (10, 100),  # Number of neurons in the first discriminator layer\n",
        "    'disc_neurons_layer2': (5, 50),    # Number of neurons in the second discriminator layer\n",
        "    'dropout_rate': (0.1, 0.5),        # Dropout rate for regularization\n",
        "}\n",
        "\n",
        "def build_generator(gen_neurons_layer1, gen_neurons_layer2, dropout_rate):\n",
        "    generator = Sequential()\n",
        "    generator.add(Dense(int(gen_neurons_layer1), input_dim=100, activation='relu'))\n",
        "    generator.add(Dense(int(gen_neurons_layer2), activation='relu'))\n",
        "    generator.add(Dense(30, activation='linear'))  # Output shape (time_steps, num_features)\n",
        "    return generator\n",
        "\n",
        "def build_discriminator(disc_neurons_layer1, disc_neurons_layer2, dropout_rate):\n",
        "    discriminator = Sequential()\n",
        "    discriminator.add(Dense(int(disc_neurons_layer1), input_dim=30, activation='relu'))\n",
        "    discriminator.add(Dense(int(disc_neurons_layer2), activation='relu'))\n",
        "    discriminator.add(Dense(1, activation='sigmoid'))\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "    return discriminator\n",
        "\n",
        "def build_gan(generator, discriminator):\n",
        "    discriminator.trainable = False\n",
        "    gan = Sequential()\n",
        "    gan.add(generator)\n",
        "    gan.add(discriminator)\n",
        "    gan.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001))\n",
        "    return gan\n",
        "\n",
        "# Assuming you have X_train, y_train as the training data\n",
        "# Replace this with your actual data\n",
        "X_train = np.random.rand(1000, 30)  # Sample data\n",
        "y_train = np.random.randint(2, size=(1000,))  # Sample labels (binary, for discriminator)\n",
        "\n",
        "# Bayesian Optimization function for GAN\n",
        "def gan_cv(gen_neurons_layer1, gen_neurons_layer2, disc_neurons_layer1, disc_neurons_layer2, dropout_rate):\n",
        "    generator = build_generator(gen_neurons_layer1, gen_neurons_layer2, dropout_rate)\n",
        "    discriminator = build_discriminator(disc_neurons_layer1, disc_neurons_layer2, dropout_rate)\n",
        "    gan = build_gan(generator, discriminator)\n",
        "\n",
        "    # Assuming you have X_train as the training data (replace with your actual data)\n",
        "    noise = np.random.rand(len(X_train), 100)  # Random noise for generator input\n",
        "    synthetic_data = generator.predict(noise)\n",
        "\n",
        "    # Combining real and synthetic data\n",
        "    X_combined = np.concatenate((X_train, synthetic_data))\n",
        "    y_combined = np.concatenate((y_train, np.ones(len(X_train))))  # Label real data as 1 and synthetic data as 0\n",
        "\n",
        "    discriminator.fit(X_combined, y_combined, epochs=10, batch_size=1500, verbose=0)\n",
        "\n",
        "    # Loss function for GAN is the negative of the discriminator's accuracy on the combined data\n",
        "    gan_loss = -discriminator.evaluate(X_combined, y_combined, verbose=0)[0]\n",
        "    return gan_loss\n",
        "\n",
        "# Perform Bayesian Optimization for GAN\n",
        "gan_optimizer = BayesianOptimization(f=gan_cv, pbounds=pbounds, random_state=42)\n",
        "gan_optimizer.maximize(init_points=5, n_iter=10)\n",
        "\n",
        "best_params = gan_optimizer.max['params']\n",
        "best_gen_neurons_layer1 = int(best_params['gen_neurons_layer1'])\n",
        "best_gen_neurons_layer2 = int(best_params['gen_neurons_layer2'])\n",
        "best_disc_neurons_layer1 = int(best_params['disc_neurons_layer1'])\n",
        "best_disc_neurons_layer2 = int(best_params['disc_neurons_layer2'])\n",
        "best_dropout_rate = best_params['dropout_rate']\n",
        "print(best_params, best_gen_neurons_layer1, best_gen_neurons_layer2, best_disc_neurons_layer1, best_disc_neurons_layer2, best_dropout_rate)\n",
        "\n",
        "# Build the final GAN with the best parameters\n",
        "best_generator = build_generator(best_gen_neurons_layer1, best_gen_neurons_layer2, best_dropout_rate)\n",
        "best_discriminator = build_discriminator(best_disc_neurons_layer1, best_disc_neurons_layer2, best_dropout_rate)\n",
        "best_gan = build_gan(best_generator, best_discriminator)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nbk2D5CFbP4",
        "outputId": "e6f0a92f-81c7-4874-fa4d-db0b6372d1df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | disc_n... | disc_n... | dropou... | gen_ne... | gen_ne... |\n",
            "-------------------------------------------------------------------------------------\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m-0.5517  \u001b[0m | \u001b[0m43.71    \u001b[0m | \u001b[0m47.78    \u001b[0m | \u001b[0m0.3928   \u001b[0m | \u001b[0m63.88    \u001b[0m | \u001b[0m12.02    \u001b[0m |\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0m-0.572   \u001b[0m | \u001b[0m24.04    \u001b[0m | \u001b[0m7.614    \u001b[0m | \u001b[0m0.4465   \u001b[0m | \u001b[0m64.1     \u001b[0m | \u001b[0m36.86    \u001b[0m |\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "| \u001b[0m3        \u001b[0m | \u001b[0m-0.6271  \u001b[0m | \u001b[0m11.85    \u001b[0m | \u001b[0m48.65    \u001b[0m | \u001b[0m0.433    \u001b[0m | \u001b[0m29.11    \u001b[0m | \u001b[0m13.18    \u001b[0m |\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "| \u001b[95m4        \u001b[0m | \u001b[95m-0.5013  \u001b[0m | \u001b[95m26.51    \u001b[0m | \u001b[95m18.69    \u001b[0m | \u001b[95m0.3099   \u001b[0m | \u001b[95m48.88    \u001b[0m | \u001b[95m18.11    \u001b[0m |\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m-0.542   \u001b[0m | \u001b[0m65.07    \u001b[0m | \u001b[0m11.28    \u001b[0m | \u001b[0m0.2169   \u001b[0m | \u001b[0m42.97    \u001b[0m | \u001b[0m25.52    \u001b[0m |\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0m-0.6154  \u001b[0m | \u001b[0m66.05    \u001b[0m | \u001b[0m11.62    \u001b[0m | \u001b[0m0.3175   \u001b[0m | \u001b[0m44.75    \u001b[0m | \u001b[0m24.91    \u001b[0m |\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "| \u001b[95m7        \u001b[0m | \u001b[95m-0.4718  \u001b[0m | \u001b[95m91.57    \u001b[0m | \u001b[95m9.741    \u001b[0m | \u001b[95m0.1589   \u001b[0m | \u001b[95m93.28    \u001b[0m | \u001b[95m39.68    \u001b[0m |\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m-0.591   \u001b[0m | \u001b[0m62.56    \u001b[0m | \u001b[0m7.15     \u001b[0m | \u001b[0m0.1143   \u001b[0m | \u001b[0m19.41    \u001b[0m | \u001b[0m15.49    \u001b[0m |\n",
            "32/32 [==============================] - 0s 4ms/step\n",
            "| \u001b[95m9        \u001b[0m | \u001b[95m-0.4194  \u001b[0m | \u001b[95m52.17    \u001b[0m | \u001b[95m49.69    \u001b[0m | \u001b[95m0.4687   \u001b[0m | \u001b[95m48.86    \u001b[0m | \u001b[95m19.2     \u001b[0m |\n",
            "32/32 [==============================] - 0s 6ms/step\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m-0.552   \u001b[0m | \u001b[0m82.68    \u001b[0m | \u001b[0m12.91    \u001b[0m | \u001b[0m0.2267   \u001b[0m | \u001b[0m34.2     \u001b[0m | \u001b[0m37.66    \u001b[0m |\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0m-0.5037  \u001b[0m | \u001b[0m54.21    \u001b[0m | \u001b[0m16.51    \u001b[0m | \u001b[0m0.3941   \u001b[0m | \u001b[0m63.62    \u001b[0m | \u001b[0m40.91    \u001b[0m |\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0m-0.5364  \u001b[0m | \u001b[0m26.53    \u001b[0m | \u001b[0m16.54    \u001b[0m | \u001b[0m0.2417   \u001b[0m | \u001b[0m48.62    \u001b[0m | \u001b[0m16.75    \u001b[0m |\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "| \u001b[0m13       \u001b[0m | \u001b[0m-0.5077  \u001b[0m | \u001b[0m89.32    \u001b[0m | \u001b[0m9.596    \u001b[0m | \u001b[0m0.3061   \u001b[0m | \u001b[0m92.39    \u001b[0m | \u001b[0m39.06    \u001b[0m |\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "| \u001b[0m14       \u001b[0m | \u001b[0m-0.4701  \u001b[0m | \u001b[0m89.21    \u001b[0m | \u001b[0m8.542    \u001b[0m | \u001b[0m0.184    \u001b[0m | \u001b[0m95.2     \u001b[0m | \u001b[0m38.64    \u001b[0m |\n",
            "32/32 [==============================] - 0s 2ms/step\n",
            "| \u001b[0m15       \u001b[0m | \u001b[0m-0.459   \u001b[0m | \u001b[0m53.89    \u001b[0m | \u001b[0m50.0     \u001b[0m | \u001b[0m0.5      \u001b[0m | \u001b[0m49.26    \u001b[0m | \u001b[0m17.87    \u001b[0m |\n",
            "=====================================================================================\n",
            "{'disc_neurons_layer1': 52.17312429569039, 'disc_neurons_layer2': 49.685783733192736, 'dropout_rate': 0.46874467003221576, 'gen_neurons_layer1': 48.863458927811735, 'gen_neurons_layer2': 19.198565462820316} 48 19 52 49 0.46874467003221576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generator = best_generator\n",
        "discriminator = best_discriminator\n",
        "gan = best_gan\n",
        "start = time.time()\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', patience=5)\n",
        "history_gan = best_gan.fit(np.random.rand(len(X_train_standardized), 100), y_train_new, epochs=10,\n",
        "                      batch_size=500, validation_split=0.2, callbacks=[es])\n",
        "end = time.time()\n",
        "duration = end - start\n",
        "minutes, seconds = divmod(duration, 60)\n",
        "print(f\"Training completed in {int(minutes)} minutes and {round(seconds, 2)} seconds.\")\n",
        "\n",
        "# Generate synthetic samples using the trained generator\n",
        "# Combine real and synthetic data\n",
        "num_synthetic_samples = len(X_train_standardized)\n",
        "noise = np.random.rand(num_synthetic_samples, 100)\n",
        "synthetic_data = generator.predict(noise)\n",
        "y_train_new_reshaped = y_train_new.reshape(-1, 1)\n",
        "y_synthetic = np.zeros((num_synthetic_samples * X_train_standardized.shape[1], 1))\n",
        "\n",
        "# Combine real and synthetic labels\n",
        "y_combined = np.concatenate((y_train_new_reshaped, y_synthetic))\n",
        "\n",
        "# Combine real and synthetic data\n",
        "X_combined = np.concatenate((X_train_standardized, synthetic_data[:, np.newaxis, :]), axis=1)\n",
        "#y_combined = np.concatenate((y_train_new, np.zeros((num_synthetic_samples, 1))))\n",
        "\n",
        "#X_combined = np.concatenate((X_train_standardized, synthetic_data))\n",
        "#y_combined = np.concatenate((y_train_new, np.zeros((num_synthetic_samples, 1))))\n",
        "\n",
        "# Train the discriminator on the combined data\n",
        "discriminator.fit(X_combined, y_combined, epochs=10, batch_size=500, validation_split=0.2)\n",
        "\n",
        "# Evaluate the GAN model\n",
        "loss, accuracy = discriminator.evaluate(X_test_standardized, y_test_new)\n",
        "y_pred = discriminator.predict(X_test_standardized)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "precision = precision_score(y_test_new, y_pred_binary)\n",
        "recall = recall_score(y_test_new, y_pred_binary)\n",
        "f1 = f1_score(y_test_new, y_pred_binary)\n",
        "roc_auc = roc_auc_score(y_test_new, y_pred)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test F1-score: {f1:.4f}\")\n",
        "print(f\"Test AUC-ROC: {roc_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "id": "LKpgyM_GFzDf",
        "outputId": "375e10df-b22e-4385-8c93-b85a82e5f94b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "318/318 [==============================] - 3s 8ms/step - loss: 0.0125 - val_loss: 0.0134\n",
            "Epoch 2/10\n",
            "318/318 [==============================] - 1s 5ms/step - loss: 0.0123 - val_loss: 0.0135\n",
            "Epoch 3/10\n",
            "318/318 [==============================] - 2s 6ms/step - loss: 0.0123 - val_loss: 0.0135\n",
            "Epoch 4/10\n",
            "318/318 [==============================] - 2s 6ms/step - loss: 0.0122 - val_loss: 0.0137\n",
            "Epoch 5/10\n",
            "318/318 [==============================] - 2s 7ms/step - loss: 0.0122 - val_loss: 0.0136\n",
            "Epoch 6/10\n",
            "318/318 [==============================] - 2s 6ms/step - loss: 0.0121 - val_loss: 0.0135\n",
            "Training completed in 0 minutes and 12.61 seconds.\n",
            "6207/6207 [==============================] - 11s 2ms/step\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-141d5d97c414>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Train the discriminator on the combined data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_combined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_combined\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Evaluate the GAN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_72\" is incompatible with the layer: expected shape=(None, 30), found shape=(None, 2, 30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "Og4OYuGt9k1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import *\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score\n",
        "from bayes_opt import BayesianOptimization\n",
        "from keras import backend as K\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "pbounds = {\n",
        "    'filters_layer1': (10, 100),   # Number of filters in the first convolutional layer\n",
        "    'filters_layer2': (5, 50),     # Number of filters in the second convolutional layer\n",
        "    'kernel_size_layer1': (3, 10),  # Kernel size for the first convolutional layer\n",
        "    'kernel_size_layer2': (3, 10),  # Kernel size for the second convolutional layer\n",
        "#    'dropout_rate': (0.1, 0.5),    # Dropout rate for regularization\n",
        "}\n",
        "\n",
        "def build_model(filters_layer1, filters_layer2, kernel_size_layer1, kernel_size_layer2, shape):\n",
        "    model = Sequential()\n",
        "    pool_size_1 = min(int(kernel_size_layer1), shape)\n",
        "    pool_size_2 = min(int(kernel_size_layer2), shape)\n",
        "    model.add(Conv1D(filters=int(filters_layer1), kernel_size=pool_size_1, activation='relu', input_shape=(30, 1)))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Conv1D(filters=int(filters_layer2), kernel_size=pool_size_2, activation='relu'))\n",
        "    model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Bayesian Optimization function\n",
        "'''def cnn_cv(filters_layer1, filters_layer2, kernel_size_layer1, kernel_size_layer2):\n",
        "    model = build_model(filters_layer1, filters_layer2, kernel_size_layer1, kernel_size_layer2)\n",
        "    # Assuming you have X_train, y_train as the training data\n",
        "    X_train_cv, X_val, y_train_cv, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "    model.fit(X_train_cv, y_train_cv, epochs=10, batch_size=1500, verbose=0)\n",
        "    val_loss = model.evaluate(X_val, y_val, verbose=0)[0]\n",
        "    return -val_loss  # Minimize the loss (negative of validation loss)\n",
        "'''\n",
        "\n",
        "def cnn_cv(filters_layer1, filters_layer2, kernel_size_layer1, kernel_size_layer2):\n",
        "    # Assuming you have X_train, y_train as the training data\n",
        "    X_train_cv, X_val, y_train_cv, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "    model = build_model(filters_layer1, filters_layer2, kernel_size_layer1, kernel_size_layer2,X_train_cv.shape[1])\n",
        "\n",
        "    # Pad the input data to make it large enough for pooling\n",
        "    X_train_cv = pad_sequences(X_train_cv, padding='post')\n",
        "    X_val = pad_sequences(X_val, padding='post')\n",
        "\n",
        "    model.fit(X_train_cv, y_train_cv, epochs=10, batch_size=1500, verbose=0)\n",
        "    val_loss = model.evaluate(X_val, y_val, verbose=0)[0]\n",
        "    return -val_loss  # Minimize the loss (negative of validation loss)\n",
        "\n",
        "\n",
        "# Perform Bayesian Optimization\n",
        "optimizer = BayesianOptimization(f=cnn_cv, pbounds=pbounds, random_state=42)\n",
        "optimizer.maximize(init_points=5, n_iter=5)\n",
        "\n",
        "best_params = optimizer.max['params']\n",
        "best_filters_layer1 = int(best_params['filters_layer1'])\n",
        "best_filters_layer2 = int(best_params['filters_layer2'])\n",
        "best_kernel_size_layer1 = int(best_params['kernel_size_layer1'])\n",
        "best_kernel_size_layer2 = int(best_params['kernel_size_layer2'])\n",
        "#best_dropout_rate = best_params['dropout_rate']\n",
        "print(best_params, best_filters_layer1, best_filters_layer2, best_kernel_size_layer1, best_kernel_size_layer2)\n",
        "model_cnn = build_model(best_filters_layer1, best_filters_layer2, best_kernel_size_layer1, best_kernel_size_layer2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "GgSwDwt_0RgG",
        "outputId": "db3c0137-6e0f-45a6-81d7-bb0a4d8df7f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|   iter    |  target   | filter... | filter... | kernel... | kernel... |\n",
            "-------------------------------------------------------------------------\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m-2.52    \u001b[0m | \u001b[0m43.71    \u001b[0m | \u001b[0m47.78    \u001b[0m | \u001b[0m8.124    \u001b[0m | \u001b[0m7.191    \u001b[0m |\n",
            "| \u001b[95m2        \u001b[0m | \u001b[95m-0.01405 \u001b[0m | \u001b[95m24.04    \u001b[0m | \u001b[95m12.02    \u001b[0m | \u001b[95m3.407    \u001b[0m | \u001b[95m9.063    \u001b[0m |\n",
            "| \u001b[0m3        \u001b[0m | \u001b[0m-0.1254  \u001b[0m | \u001b[0m64.1     \u001b[0m | \u001b[0m36.86    \u001b[0m | \u001b[0m3.144    \u001b[0m | \u001b[0m9.789    \u001b[0m |\n",
            "| \u001b[0m4        \u001b[0m | \u001b[0m-0.8678  \u001b[0m | \u001b[0m84.92    \u001b[0m | \u001b[0m14.56    \u001b[0m | \u001b[0m4.273    \u001b[0m | \u001b[0m4.284    \u001b[0m |\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m-0.1336  \u001b[0m | \u001b[0m37.38    \u001b[0m | \u001b[0m28.61    \u001b[0m | \u001b[0m6.024    \u001b[0m | \u001b[0m5.039    \u001b[0m |\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0m-0.01482 \u001b[0m | \u001b[0m22.94    \u001b[0m | \u001b[0m12.32    \u001b[0m | \u001b[0m4.14     \u001b[0m | \u001b[0m9.236    \u001b[0m |\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m-0.02599 \u001b[0m | \u001b[0m39.89    \u001b[0m | \u001b[0m13.19    \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m3.0      \u001b[0m |\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m-0.01698 \u001b[0m | \u001b[0m55.18    \u001b[0m | \u001b[0m17.86    \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m10.0     \u001b[0m |\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m-0.1844  \u001b[0m | \u001b[0m10.0     \u001b[0m | \u001b[0m29.11    \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m3.0      \u001b[0m |\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m-0.07622 \u001b[0m | \u001b[0m85.34    \u001b[0m | \u001b[0m49.12    \u001b[0m | \u001b[0m3.0      \u001b[0m | \u001b[0m10.0     \u001b[0m |\n",
            "=========================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-83694e6aac13>\u001b[0m in \u001b[0;36m<cell line: 65>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0mbest_kernel_size_layer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kernel_size_layer1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mbest_kernel_size_layer2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'kernel_size_layer2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mbest_dropout_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dropout_rate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_filters_layer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_filters_layer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_kernel_size_layer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_kernel_size_layer2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0mmodel_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_filters_layer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_filters_layer2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_kernel_size_layer1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_kernel_size_layer2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'dropout_rate'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(best_params, best_filters_layer1, best_filters_layer2, best_kernel_size_layer1, best_kernel_size_layer2)\n",
        "model_cnn = build_model(best_filters_layer1, best_filters_layer2, best_kernel_size_layer1, best_kernel_size_layer2,5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uh_HDnUG4aGO",
        "outputId": "c9cd68bd-bb52-4653-bb86-7d468fbfecb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'filters_layer1': 24.041677639819287, 'filters_layer2': 12.01975341512912, 'kernel_size_layer1': 3.406585285177396, 'kernel_size_layer2': 9.063233020424546} 24 12 3 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start = time.time()\n",
        "es = EarlyStopping(monitor='val_loss', mode='min',patience=5)\n",
        "history_cnn = model_cnn.fit(X_train, y_train, epochs=10,batch_size=500,validation_data=(X_test,y_test),shuffle=False, callbacks=[es])\n",
        "end = time.time()\n",
        "duration = end - start\n",
        "minutes, seconds = divmod(duration, 60)\n",
        "print(f\"Training completed in {int(minutes)} minutes and {round(seconds, 2)} seconds.\")\n",
        "\n",
        "\n",
        "# Evaluate on the test set\n",
        "loss, accuracy = model_cnn.evaluate(X_test, y_test)\n",
        "y_pred = model_cnn.predict(X_test)\n",
        "y_pred_binary = (y_pred > 0.5).astype(int)  # Convert probabilities to binary predictions\n",
        "print(y_pred.shape)\n",
        "y_new_test= y_test_new.reshape(-1)  # or y_test_new = np.squeeze(y_test_new)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "precision = precision_score(y_new_test, y_pred_binary)\n",
        "recall = recall_score(y_new_test, y_pred_binary)\n",
        "f1 = f1_score(y_new_test, y_pred_binary)\n",
        "roc_auc = roc_auc_score(y_new_test, y_pred)\n",
        "\n",
        "# Print the metrics\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Test Precision: {precision:.4f}\")\n",
        "print(f\"Test Recall: {recall:.4f}\")\n",
        "print(f\"Test F1-score: {f1:.4f}\")\n",
        "print(f\"Test AUC-ROC: {roc_auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "gzmFd0m94rH3",
        "outputId": "d38a37d0-6b6f-43c3-eb06-2e7ac3f4cdcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-b2dce4dd926c>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory_cnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mduration\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    864\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 866\u001b[0;31m         raise ValueError(\"Creating variables on a non-first call to a function\"\n\u001b[0m\u001b[1;32m    867\u001b[0m                          \" decorated with tf.function.\")\n\u001b[1;32m    868\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Creating variables on a non-first call to a function decorated with tf.function."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import *\n",
        "from keras.optimizers import Adagrad,Adam\n",
        "from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score\n",
        "def build_model(neurons_layer1,neurons_layer2,dropout_rate):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(int(neurons_layer1), input_shape=(1,30), dropout=float(dropout_rate), recurrent_dropout=0, return_sequences = True))  # Input shape is (time_steps, num_features)\n",
        "  model.add(LSTM(int(neurons_layer2),  return_sequences = True))\n",
        "  model.add(LSTM(1))\n",
        "  model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])#, Precision(), Recall()])\n",
        "  return model\n",
        "model_lstm = build_model(97,37,0.249816)"
      ],
      "metadata": {
        "id": "BEAzf2i8ktUS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmm_zQmMYNFe",
        "outputId": "af08cd08-511b-4504-dc0c-bbdf66298d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_30 (LSTM)              (None, 1, 87)             41064     \n",
            "                                                                 \n",
            " lstm_31 (LSTM)              (None, 1, 23)             10212     \n",
            "                                                                 \n",
            " lstm_32 (LSTM)              (None, 1)                 100       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51376 (200.69 KB)\n",
            "Trainable params: 51376 (200.69 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "r87AKOlEYxiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import *\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "import autokeras as ak\n",
        "\n",
        "# Assuming you have X_train and y_train as your training data\n",
        "# Split the data into training and validation sets\n",
        "#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the AutoKeras classifier\n",
        "clf = ak.StructuredDataClassifier(overwrite = True,max_trials=2)\n",
        "\n",
        "# Search for the best architecture and hyperparameters\n",
        "X_train_cv, X_val, y_train_cv, y_val = train_test_split(X_train_standardized, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "clf.fit(X_train_cv, y_train_cv, epochs=10, validation_data=(X_val, y_val))\n",
        "\n",
        "# Get the best model found by AutoKeras\n",
        "model_lstm = clf.export_model()\n",
        "\n",
        "# Compile the model with the desired optimizer and metrics\n",
        "model_lstm.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])\n",
        "\n",
        "# Evaluate the best model on your test data\n",
        "results = model_lstm.evaluate(X_test, y_test)\n",
        "print(\"Test loss:\", results[0])\n",
        "print(\"Test accuracy:\", results[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aNL-LMEBO4EX",
        "outputId": "1459a628-2095-405f-d053-067a6c6acb17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 2 Complete [00h 05m 56s]\n",
            "val_accuracy: 0.9994461536407471\n",
            "\n",
            "Best val_accuracy So Far: 0.9994461536407471\n",
            "Total elapsed time: 00h 11m 50s\n",
            "Epoch 1/10\n",
            "4966/4966 [==============================] - 31s 6ms/step - loss: 0.0100 - accuracy: 0.9981 - val_loss: 0.0043 - val_accuracy: 0.9994\n",
            "Epoch 2/10\n",
            "4966/4966 [==============================] - 29s 6ms/step - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
            "Epoch 3/10\n",
            "4966/4966 [==============================] - 30s 6ms/step - loss: 0.0027 - accuracy: 0.9995 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
            "Epoch 4/10\n",
            "4966/4966 [==============================] - 27s 5ms/step - loss: 0.0024 - accuracy: 0.9995 - val_loss: 0.0038 - val_accuracy: 0.9993\n",
            "Epoch 5/10\n",
            "4966/4966 [==============================] - 30s 6ms/step - loss: 0.0022 - accuracy: 0.9995 - val_loss: 0.0039 - val_accuracy: 0.9994\n",
            "Epoch 6/10\n",
            "4966/4966 [==============================] - 28s 6ms/step - loss: 0.0021 - accuracy: 0.9996 - val_loss: 0.0041 - val_accuracy: 0.9994\n",
            "Epoch 7/10\n",
            "4966/4966 [==============================] - 29s 6ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0043 - val_accuracy: 0.9993\n",
            "Epoch 8/10\n",
            "4966/4966 [==============================] - 30s 6ms/step - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0043 - val_accuracy: 0.9993\n",
            "Epoch 9/10\n",
            "4966/4966 [==============================] - 43s 9ms/step - loss: 0.0018 - accuracy: 0.9996 - val_loss: 0.0041 - val_accuracy: 0.9994\n",
            "Epoch 10/10\n",
            "4966/4966 [==============================] - 31s 6ms/step - loss: 0.0017 - accuracy: 0.9996 - val_loss: 0.0044 - val_accuracy: 0.9994\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2660/2660 [==============================] - 10s 4ms/step - loss: 428.6035 - accuracy: 0.9983\n",
            "Test loss: 428.6034851074219\n",
            "Test accuracy: 0.9983317255973816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_lstm.summary()"
      ],
      "metadata": {
        "id": "LCp04ZafP37P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ray[tune]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIhbr2aPogSR",
        "outputId": "2dc99e33-2413-4e9d-b58e-0bcd7d858541"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ray[tune] in /usr/local/lib/python3.10/dist-packages (2.6.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (8.1.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.12.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (4.3.3)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.0.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (23.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (6.0.1)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.27.1)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.56.2)\n",
            "Requirement already satisfied: numpy>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.22.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (1.5.3)\n",
            "Requirement already satisfied: tensorboardX>=1.9 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (2.6.2)\n",
            "Requirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[tune]) (9.0.0)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray[tune]) (0.19.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ray[tune]) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray[tune]) (3.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->ray[tune]) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ray\n",
        "from ray import tune\n",
        "from keras.layers import *\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import precision_score, recall_score, roc_auc_score, f1_score\n",
        "from bayes_opt import BayesianOptimization\n",
        "from keras import backend as K\n",
        "\n",
        "# Your existing code goes here...\n",
        "def build_model(neurons_layer1,neurons_layer2,dropout_rate):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(int(neurons_layer1), input_shape=(1,30), dropout=float(dropout_rate), recurrent_dropout=0, return_sequences = True))  # Input shape is (time_steps, num_features)\n",
        "  model.add(LSTM(int(neurons_layer2),  return_sequences = True))\n",
        "  model.add(LSTM(1))\n",
        "  model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['precision_score'])#, Precision(), Recall()])\n",
        "  return model\n",
        "\n",
        "def lstm_cv(neurons_layer1, neurons_layer2, dropout_rate,X_train_standardized, y_train_new, config):\n",
        "    model = build_model(neurons_layer1, neurons_layer2, dropout_rate)\n",
        "    # Assuming you have X_train, y_train as the training data\n",
        "   # X_train_cv, X_val, y_train_cv, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "    model.fit(X_train_standardized, y_train_new, epochs=config['epochs'], batch_size=config['batch_size'], verbose=0)\n",
        "    val_loss = model.evaluate(X_test_standardized, y_test_new, verbose=0)[0]\n",
        "    tune.report(val_loss=-val_loss)  # Use tune.report() to report the metric\n",
        "\n",
        "# Perform Bayesian Optimization\n",
        "#optimizer = BayesianOptimization(f=lstm_cv, pbounds=pbounds, random_state=42)\n",
        "#optimizer.maximize(init_points=5, n_iter=5)\n",
        "\n",
        "# Best hyperparameters\n",
        "#best_params = optimizer.max['params']\n",
        "#best_neuron_layer1 = int(best_params['neurons_layer1'])\n",
        "#best_neuron_layer2 = int(best_params['neurons_layer2'])\n",
        "#best_dropout_rate = best_params['dropout_rate']\n",
        "\n",
        "# Now, let's use Ray Tune for hyperparameter optimization\n",
        "X_train_standardized_id = ray.put(X_train_standardized)\n",
        "#X_train_new_id = ray.put(X_train)\n",
        "y_train_new_id = ray.put(y_train_new)\n",
        "\n",
        "def trainable_function(config):\n",
        "    lstm_cv(best_neuron_layer1, best_neuron_layer2, best_dropout_rate, X_train_standardized_id, y_train_new_id , config)\n",
        "\n",
        "# Configuration space for Tune\n",
        "config_space = {\n",
        "    'neurons_layer1': tune.grid_search(list(range(10, 101))),\n",
        "    'neurons_layer2': tune.grid_search(list(range(5, 51))),\n",
        "    'dropout_rate': tune.grid_search(list(np.linspace(0.1, 0.5, num=10))),\n",
        "    'epochs': 10,\n",
        "    'batch_size': 1500\n",
        "}\n",
        "\n",
        "# Initialize Ray\n",
        "if ray.is_initialized():\n",
        "    ray.shutdown()\n",
        "ray.init()\n",
        "\n",
        "# Start hyperparameter search using Tune\n",
        "analysis = tune.run(\n",
        "    trainable_function,\n",
        "    config=config_space,\n",
        "    num_samples=5,\n",
        ")\n",
        "\n",
        "# Access best hyperparameters and results\n",
        "best_hyperparameters = analysis.get_best_config(metric='val_loss', mode='min')\n",
        "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
        "\n",
        "# You can access the results using `analysis` object\n",
        "print(\"Best Validation Loss:\", analysis.best_result['val_loss'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "id": "Z-88ZayQon0p",
        "outputId": "d6d2fe9a-1255-4d93-fc5c-c07ece40f734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2023-08-02 09:58:03,216\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "2023-08-02 09:58:12,874\tINFO worker.py:1621 -- Started a local Ray instance.\n",
            "2023-08-02 09:58:17,077\tINFO tune.py:666 -- [output] This will use the new output engine with verbosity 2. To disable the new output and use the legacy output engine, set the environment variable RAY_AIR_NEW_OUTPUT=0. For more information, please see https://github.com/ray-project/ray/issues/36949\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-bdccbd6fdf57>\u001b[0m in \u001b[0;36m<cell line: 59>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# Start hyperparameter search using Tune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m analysis = tune.run(\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0mtrainable_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_space\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, search_alg, scheduler, checkpoint_config, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, chdir_to_trial_dir, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, raise_on_failed_trial, callbacks, max_concurrent_trials, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, checkpoint_keep_all_ranks, checkpoint_upload_from_workers, trial_executor, local_dir, _experiment_checkpoint_dir, _remote, _remote_string_queue, _entrypoint)\u001b[0m\n\u001b[1;32m    855\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    856\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 857\u001b[0;31m             experiments[i] = Experiment(\n\u001b[0m\u001b[1;32m    858\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m                 \u001b[0mrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/experiment/experiment.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, run, stop, time_budget_s, config, resources_per_trial, num_samples, storage_path, _experiment_checkpoint_dir, sync_config, checkpoint_config, trial_name_creator, trial_dirname_creator, log_to_file, export_formats, max_failures, restore, local_dir)\u001b[0m\n\u001b[1;32m    217\u001b[0m                 )\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_identifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpc_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatusCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRESOURCE_EXHAUSTED\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/experiment/experiment.py\u001b[0m in \u001b[0;36mregister_if_needed\u001b[0;34m(cls, run_object)\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trainable_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m             \u001b[0mregister_trainable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPicklingError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             extra_msg = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/registry.py\u001b[0m in \u001b[0;36mregister_trainable\u001b[0;34m(name, trainable, warn)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0missubclass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Second argument must be convertable to Trainable\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m     \u001b[0m_global_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTRAINABLE_CLASS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/tune/registry.py\u001b[0m in \u001b[0;36mregister\u001b[0;34m(self, category, key, value)\u001b[0m\n\u001b[1;32m    235\u001b[0m                 \u001b[0;34m\"Unknown category {} not among {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mKNOWN_CATEGORIES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             )\n\u001b[0;32m--> 237\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_flush\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps_debug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_internal_kv_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/cloudpickle/__init__.py\u001b[0m in \u001b[0;36mdumps_debug\u001b[0;34m(obj, *args, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdumps_debug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPicklingError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RAY_PICKLE_VERBOSE_DEBUG\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_callback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuffer_callback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/cloudpickle/cloudpickle_fast.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    731\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"recursion\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ray/_private/serialization.py\u001b[0m in \u001b[0;36mobject_ref_reducer\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_connected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_contained_object_ref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             obj, owner_address, object_status = worker.core_worker.serialize_object_ref(\n\u001b[0m\u001b[1;32m    133\u001b[0m                 \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             )\n",
            "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.serialize_object_ref\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: An application is trying to access a Ray object whose owner is unknown(00ffffffffffffffffffffffffffffffffffffff0100000001e1f505). Please make sure that all Ray objects you are trying to access are part of the current Ray session. Note that object IDs generated randomly (ObjectID.from_random()) or out-of-band (ObjectID.from_binary(...)) cannot be passed as a task argument because Ray does not know which task created them. If this was not how your object ID was generated, please file an issue at https://github.com/ray-project/ray/issues/"
          ]
        }
      ]
    }
  ]
}